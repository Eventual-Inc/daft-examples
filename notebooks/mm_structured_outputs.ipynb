{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8n0CLwwb5dZ"
      },
      "source": [
        "# Multimodal Structured Outputs with Daft, Gemma-3, and vLLM\n",
        "\n",
        "*An end-to-end example of **Multimodal Structured Outputs** with Daft's high performance data engine.*\n",
        "\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/everettVT/daft-examples-1/blob/main/notebooks/mm_structured_outputs.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "\n",
        "## Introduction\n",
        "\n",
        "**Structured Outputs** refers to a family of features that enables language models to respond in a constrained format. While language models continue to improve and demonstrate emergent abilities, their unpredictable nature make them difficult to integrate them with traditional software systems. Most real-world AI uses-cases leverage structured outputs to some extext, whether that be to execute tool calls or adhere to Pydantic Models. The underlying technology that makes structured outputs is called guided decoding.\n",
        "\n",
        "Guided decoding uses logits to control the output of a language model by adjusting the probabilities of the next possible tokens to enforce constraints or guide the generation process. This can be done through various methods, such as applying a logit bias to penalize or promote specific tokens, filtering invalid tokens based on rules like a Finite State Machine (FSM), or by using more advanced techniques to interact with the model's internal probability distribution.\n",
        "\n",
        "Structured Outputs strategies consist of 5 strategies that define the desired output type:\n",
        "\n",
        "- Basic Python Types: `int`, `float`, `bool`...\n",
        "- Multiple Choices: using `Literal` or `Enum`\n",
        "- JSON Schemas: using Pydantic models or dataclasses\n",
        "- Regex\n",
        "- Context-free Grammars\n",
        "\n",
        "While there are tremendous number of examples in pure python, few tutorials exist that demonstrate structured outputs within a large-scale processing context. Even fewer, if any, examples exist that demonstrate how to run batch structured outputs with multimodal data on your own inference server. Here, we will walk you through the entire process, using your own OpenAI-compatible server using [vLLM](https://docs.vllm.ai/en/v0.6.3.post1/serving/openai_compatible_server.html).\n",
        "\n",
        "Large scale multimodal structured outptus is a real world use-case that every enterprise team faces when attempting to work with massive amounts of internal/private data. These teams face significant hurdles with traditional tooling, especially for cutting-edge uses cases like batch tool calls for background agents or reinforcement learning with verifiable rewards.\n",
        "\n",
        "Daft's unified multimodal data processing engine is purpose built to support workloads like this and is rapidly becoming the default engine of choice for teams deploying frontier AI solutions in production.\n",
        "\n",
        "In this notebook, we will leverage Daft to evaluate the image understanding accuracy of [Google's Gemma‑3‑4b‑it](https://ai.google.dev/gemma/docs/core) using HugginFace's [the_cauldron dataset](https://huggingface.co/datasets/HuggingFaceM4/the_cauldron). By the end of this notebook, you will be ready to implement your own distributed batch structured outputs pipeline with a copy-paste script you can use in your own environment.\n",
        "\n",
        "> NOTE:\n",
        "  This Notebook contains an advanced path where you can use vLLM as your inference solution. In this case, Google Colab's A100 GPU instance is recommended. Additionally, in order to access to [google/gemma-3-4b-it](https://huggingface.co/google/gemma-3-4B-it) you will need accept Google's usage policy and authenticate with HuggingFace.\n",
        "\n",
        "### Table of Contents\n",
        "\n",
        "1. [Setup](#1-setup)\n",
        "2. [Choose an Inference Solution](#2-choose-an-inference-solution)\n",
        "3. [Sanity Check OpenAI Client Requests](#3-sanity-check-openai-client-requests)\n",
        "4. [Dataset Preprocessing](#4-dataset-preprocessing)\n",
        "5. [Multimodal Structured Outputs with `prompt`](#5-multimodal-inference-with-structured-outputs)\n",
        "6. [Post Processing and Analysis](#)\n",
        "7. [Evaluation]\n",
        "8. [Conclusion]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8P5OPUCtIkO"
      },
      "source": [
        "## 1. Setup\n",
        "\n",
        "### Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kxDFf_3iZyzq"
      },
      "outputs": [],
      "source": [
        "!pip install -q daft openai numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G311EzbhZyzq"
      },
      "source": [
        "### Configure Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N4VKXxT1Zyzq"
      },
      "outputs": [],
      "source": [
        "import daft\n",
        "\n",
        "# PICK ONE\n",
        "MODEL_ID = \"google/gemma-3-4b-it\"        # vLLM & OpenRouter\n",
        "# MODEL_ID = \"google/gemma-3-4b-it:free\" # OpenRouter free version (Low Rate Limits)\n",
        "# MODEL_ID = \"google/gemma-3-4b\"         # LM Studio\n",
        "DATASET_URI = \"HuggingFaceM4/the_cauldron\"\n",
        "\n",
        "# Inference Parameters\n",
        "ROW_LIMIT = 100\n",
        "TEMPERATURE = 0.1\n",
        "CONCURRENCY = 4\n",
        "BATCH_SIZE = 32"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgbZr0tDZyzq"
      },
      "source": [
        "## 2. Choose an Inference Solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtIxKYwIrjG7"
      },
      "source": [
        "### Option 1: Use OpenRouter (provider)\n",
        "\n",
        "[OpenRouter](https://openrouter.ai/models?max_price=0.5&order=top-weekly) has model endpoints for [google/gemma-3-4b-it](https://openrouter.ai/google/gemma-3-4b-it). That means if you don't have access to an A100 GPU or a PRO Google Colab subscription, you can still walk through this notebook without spinning up a production vLLM server."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XqeD8eIDZyzq"
      },
      "outputs": [],
      "source": [
        "!export OPENROUTER_API_KEY=..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YAvrx9-oZyzr"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "OPENAI_API_KEY = os.environ.get(\"OPENROUTER_API_KEY\")\n",
        "OPENAI_BASE_URL = \"https://openrouter.ai/api/v1/\"\n",
        "MODEL_ID = \"google/gemma-3-4b-it\"\n",
        "MODEL_ID = \"google/gemma-3-4b\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGD6M0PFZyzr"
      },
      "source": [
        "### Option 2: Use LM Studio (local)\n",
        "\n",
        "Similarly, if you are running on a Mac, [LMStudio](https://lmstudio.ai/) is a particularly attractive option since the [gemma-3-4b](https://lmstudio.ai/models/google/gemma-3-4b) only takes up 2 GB of storage with both [MLX](https://github.com/ml-explore/mlx) and [GGUF](https://huggingface.co/docs/hub/en/gguf) variants. Daft already supports LM Studio as a provider which means you can take advantage of [Apple Metal Performance Shaders](https://developer.apple.com/documentation/metalperformanceshaders) on your local machine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tpQNBtrjZyzr"
      },
      "outputs": [],
      "source": [
        "OPENAI_API_KEY = \"none\"\n",
        "OPENAI_BASE_URL = \"http://127.0.0.1:1234/v1\"\n",
        "MODEL_ID = \"google/gemma-3-4b\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mO_WTFI0Zyzr"
      },
      "source": [
        "### Option 2: Launch vLLM OpenAI Compatible Server (Advanced)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9jxkcTMZyzr"
      },
      "source": [
        "#### Install vLLM\n",
        "\n",
        "After you install vllm you will be prompted to restart the session, then proceed to the next step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QZuYcV6s7R43"
      },
      "outputs": [],
      "source": [
        "!pip install -q vllm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXBgajW4ssGZ"
      },
      "source": [
        "#### Log in to HF for access google/gemma-3-4b-it\n",
        "\n",
        "The [google/gemma-3-4b-it repository](https://huggingface.co/google/gemma-3-4B-it) is publicly accessible, but will need to login to HuggingFace and accept Google's conditions to access its files and content. Requests are processed immediately."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JIC6JtSPseqB"
      },
      "outputs": [],
      "source": [
        "!hf auth login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pf6efT4MZyzr"
      },
      "source": [
        "#### Launch vLLM OpenAI Compatible Server\n",
        "\n",
        "Run the following vllm cli command in your terminal\n",
        "\n",
        "If you are in Google Colab, you can open a terminal by clicking the terminal icon in the bottom left of the ui.\n",
        "\n",
        "```bash\n",
        " python -m vllm.entrypoints.openai.api_server \\\n",
        "  --model google/gemma-3-4b-it \\\n",
        "  --enable-chunked-prefill \\\n",
        "  --guided-decoding-backend guidance \\\n",
        "  --dtype bfloat16 \\\n",
        "  --gpu-memory-utilization 0.85 \\\n",
        "  --host 0.0.0.0 --port 8000\n",
        "```\n",
        "\n",
        "* This config is optimized for Google Colab's A100 instance and gemma-3-4b-it.\n",
        "* For vLLM online serving, set `api_key = \"none\"` and `base_url = \"http://0.0.0.0:8000/v1\"`\n",
        "* Server readiness may take ~7–8 minutes; ‘guided_choice’ requires guided decoding enabled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ch_JxOyeZyzr"
      },
      "outputs": [],
      "source": [
        "OPENAI_API_KEY = \"none\"\n",
        "OPENAI_BASE_URL = \"http://0.0.0.0:8000/v1\"\n",
        "MODEL_ID = \"google/gemma-3-4b-it\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqjRsfxH891x"
      },
      "source": [
        "## 3. Sanity Check OpenAI Client Requests\n",
        "\n",
        "Configuring an inference server on a new model can be a long and painful process. Adding support for Images and Guided Decoding are not standard options, and tuning a particular model to specific hardware takes multiple iterations to get right. Along the way, we need to make sure our inference server is working across all of the types of requests we expect to need to support.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0yL-XBvtFv0Q"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(api_key=OPENAI_API_KEY, base_url=OPENAI_BASE_URL)\n",
        "\n",
        "# Test Client connects to Server\n",
        "result = client.models.list()\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FmywS9kiTZLO"
      },
      "outputs": [],
      "source": [
        "# Test Simple Text Completion\n",
        "chat_completion = client.chat.completions.create(\n",
        "    messages=[{\"role\": \"user\", \"content\": \"How many strawberries are in the word r?\"}],\n",
        "    model=MODEL_ID,\n",
        ")\n",
        "\n",
        "result = chat_completion.choices[0].message.content\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "071agIW4Qg4n"
      },
      "outputs": [],
      "source": [
        "# Test Structured Output\n",
        "completion = client.chat.completions.create(\n",
        "    model=MODEL_ID,\n",
        "    messages=[{\"role\": \"user\", \"content\": \"Classify this sentiment: Daft is wicked fast!\"}],\n",
        "    extra_body={\"guided_choice\": [\"positive\", \"negative\"]},\n",
        ")\n",
        "print(completion.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rjj5_JFQczgt"
      },
      "outputs": [],
      "source": [
        "# Test Image Understanding\n",
        "IMAGE_URL = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg\"\n",
        "\n",
        "completion = client.chat.completions.create(\n",
        "    model=MODEL_ID,\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}]},\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"image_url\", \"image_url\": {\"url\": IMAGE_URL}},\n",
        "                {\"type\": \"text\", \"text\": \"Describe this image in detail.\"},\n",
        "            ],\n",
        "        },\n",
        "    ],\n",
        ")\n",
        "print(completion.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osXVHWQ7eHim"
      },
      "source": [
        "### Test Combining Image Inputs with Structured Output\n",
        "\n",
        "We can play with prompting/structured outputs to understand how prompting and structured outputs can affect results.\n",
        "\n",
        "Try commenting out the `response_model` argument or the third text prompt to see how results change.\n",
        "\n",
        "vLLM also supports a simpler usage pattern of `extra_body={guided_choice:[\"A\",\"B\",\"C\",\"D\"]}`, but for compatibility with OpenRouter we use the Pydantic Json Schema approach."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ril6tbiTeLn4"
      },
      "outputs": [],
      "source": [
        "from enum import Enum\n",
        "\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import Literal\n",
        "\n",
        "# Define a pydantic model\n",
        "class ChoiceResponse(BaseModel):\n",
        "    choice: str = Field(..., description=\"Provide the letter of the correct choice with no other text.\")\n",
        "\n",
        "\n",
        "# Test Image Understanding\n",
        "completion = client.chat.completions.create(\n",
        "    model=MODEL_ID,\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}]},\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"image_url\", \"image_url\": {\"url\": IMAGE_URL}},\n",
        "                {\n",
        "                    \"type\": \"text\",\n",
        "                    \"text\": \"Which insect is portrayed in the image: A. Ladybug, B. Beetle, C. Bee, D. Wasp \",\n",
        "                },\n",
        "                # {\"type\": \"text\", \"text\": \"Answer with only the letter from the multiple choice. \"} # Try comment me out\n",
        "            ],\n",
        "        },\n",
        "    ],\n",
        "    response_format={\n",
        "        \"type\": \"json_schema\",\n",
        "        \"json_schema\": {\n",
        "            \"name\": \"math-response\",\n",
        "            \"schema\": ChoiceResponse.model_json_schema(),\n",
        "        },\n",
        "    },\n",
        ")\n",
        "response = completion.choices[0].message.content\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upptueJcZyzr"
      },
      "outputs": [],
      "source": [
        "# Pydantic Valiation\n",
        "choice_obj = ChoiceResponse.model_validate_json(response)\n",
        "print(choice_obj)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYfIxkMxf8Nc"
      },
      "source": [
        "## 4: Dataset Preprocessing\n",
        "\n",
        "[HuggingFaceM4/the_cauldron](https://huggingface.co/datasets/HuggingFaceM4/the_cauldron/viewer?views%5B%5D=ai2d) is a massive collection of 50 vision-language dataset spanning millions of rows across:\n",
        "\n",
        "1. General visual question answering\n",
        "2. OCR document understanding & text transcription\n",
        "3. Chart/figure understanding\n",
        "4. Table understanding\n",
        "5. Reasoning, logic, maths\n",
        "6. Textbook/academic questions\n",
        "7. Differences between 2 images\n",
        "8. Screenshot to code\n",
        "\n",
        "For now we will begin with General visual Q&A subset AI2D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "27J-inULb4kn"
      },
      "outputs": [],
      "source": [
        "import daft\n",
        "\n",
        "df_raw = daft.read_huggingface(\"HuggingFaceM4/the_cauldron/ai2d\").collect()\n",
        "df_raw.show(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEVwwhXDPohX"
      },
      "source": [
        " Taking a look at the schema we can see the familiar messages nested datatype we are used to in chat completions inside the `texts` column\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IZHGfSd_74K9"
      },
      "outputs": [],
      "source": [
        "print(df_raw.schema())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpxeWOz8P4pO"
      },
      "source": [
        "Lets decode the image bytes to see a preview of the images and add one more column for the base64 encoding.\n",
        "\n",
        "Note: You can click on any cell to preview its contents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zFmi01VkPmUA"
      },
      "outputs": [],
      "source": [
        "from daft import col\n",
        "\n",
        "df_img = df_raw.explode(col(\"images\")).with_columns(\n",
        "    {\n",
        "        \"image\": col(\"images\").struct.get(\"bytes\").image.decode(),          # For viewing the images\n",
        "        \"image_base64\": col(\"images\").struct.get(\"bytes\").encode(\"base64\"), # For openai requests\n",
        "    }\n",
        ")\n",
        "df_img.show(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJFwixJpQTAn"
      },
      "source": [
        "#### Preprocessing the 'texts' column to extract Question, Choices, and Answer Columns\n",
        "\n",
        "Copy/Pasting an entry from the `texts` column yields an openai messages list of dicts of the form:\n",
        "\n",
        "```python\n",
        "[{\n",
        "    \"user\": \"\"\"Question:\n",
        "            \n",
        "        From the above food web diagram, what cause kingfisher to increase\n",
        "\n",
        "        Choices:\n",
        "            A. decrease in fish\n",
        "            B. decrease in water boatman\n",
        "            C. increase in fish\n",
        "            D. increase in algae\n",
        "\n",
        "        Answer with the letter.\"\"\",\n",
        "\n",
        "    \"assistant\": \"Answer: C\",\n",
        "    \"source\": \"AI2D\",\n",
        "}, ...]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HMW536eHUJ6Y"
      },
      "outputs": [],
      "source": [
        "# Explode the List of Dicts inside \"texts\" to extract \"user\" and \"assistant\" messages\n",
        "df_text = df_img.explode(col(\"texts\")).collect()\n",
        "\n",
        "# Extract User and Assistant Messages\n",
        "df_text = df_text.with_columns(\n",
        "    {\"user\": df_text[\"texts\"].struct.get(\"user\"), \"assistant\": df_text[\"texts\"].struct.get(\"assistant\")}\n",
        ").collect()\n",
        "df_text.select(\"texts\",\"image\", ).show(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brCTE8koRhfb"
      },
      "source": [
        "We can also go above an beyond to parse each text input into individual question, choices, and answer columns.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JUD64rVnkWYE"
      },
      "outputs": [],
      "source": [
        "# Parsing \"user\" and \"assistant\" messages for question, choices, and answer\"\"\n",
        "df_prepped = df_text.with_columns(\n",
        "    {\n",
        "        \"question\": col(\"user\")\n",
        "        .str.extract(r\"(?s)Question:\\s*(.*?)\\s*Choices:\")\n",
        "        .str.replace(\"Choices:\", \"\")\n",
        "        .str.replace(\"Question:\", \"\"),\n",
        "        \"choices_string\": col(\"user\")\n",
        "        .str.extract(r\"(?s)Choices:\\s*(.*?)\\s*Answer?\\.?\")\n",
        "        .str.replace(\"Choices:\\n\", \"\")\n",
        "        .str.replace(\"Answer\", \"\"),\n",
        "        \"answer\": col(\"assistant\").str.extract(r\"Answer:\\s*(.*)$\").str.replace(\"Answer:\", \"\"),\n",
        "    }\n",
        ").collect()\n",
        "\n",
        "df_prepped.show(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkVMvG9cg30_"
      },
      "source": [
        "## 5. Multimodal Inference with Structured Outputs\n",
        "\n",
        "Now we will move on to scaling our OpenAI client calls with Daft's new `prompt` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mr-nh8scZyzs"
      },
      "outputs": [],
      "source": [
        "daft.set_provider(\"openai\", api_key=OPENAI_API_KEY, base_url=OPENAI_BASE_URL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t4G_yjdNdayW"
      },
      "outputs": [],
      "source": [
        "from daft import col\n",
        "from daft.functions import format, prompt\n",
        "import time\n",
        "\n",
        "start = time.time()\n",
        "df = df_prepped.with_column(\n",
        "    \"result\",\n",
        "    prompt(\n",
        "        [col(\"image\"), col(\"user\")],\n",
        "        system_message = \"Observe the attached image and respond to the multiple choice question with just the letter corresponding to the correct answer.\",\n",
        "        model = MODEL_ID,\n",
        "        provider = \"openai\",\n",
        "        use_chat_completions=True,\n",
        "        extra_body={\"guided_choice\": [\"A\", \"B\", \"C\", \"D\"]},\n",
        "    )\n",
        ").collect()\n",
        "end = time.time()\n",
        "print(f\"Processed {df.count_rows()} rows in {end-start} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.show()"
      ],
      "metadata": {
        "id": "zdOULnFwpfV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVgjnPjfkEcz"
      },
      "source": [
        "___\n",
        "# Analysis\n",
        "Evaluating Gemma-3's performance on image understanding by comparing structured output responses to the answer."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.with_column(\"is_correct\", col(\"result\").lstrip().rstrip() == col(\"answer\").lstrip().rstrip()) # strip whitespace"
      ],
      "metadata": {
        "id": "69XbKxrHqsF8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b_GdTLRm_vgx"
      },
      "outputs": [],
      "source": [
        "pass_fail_rate = df.where(col(\"is_correct\")).count_rows() / df.count_rows()\n",
        "print(f\"Pass/Fail Rate: {pass_fail_rate}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets investigate some of the Failures\n",
        "\n",
        "df_failures = df.where(col(\"is_correct\") == False).select(\"user\", \"image\", \"answer\", \"result\").show(8)"
      ],
      "metadata": {
        "id": "CPKYUSpHrM3a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0nlo1hCzM1_l"
      },
      "outputs": [],
      "source": [
        "# How does these results compare without images?\n",
        "start = time.time()\n",
        "df_no_image = df_prepped.with_column(\n",
        "    \"result\",\n",
        "    prompt(\n",
        "        col(\"user\"),\n",
        "        system_message = \"Observe the attached image and respond to the multiple choice question with just the letter corresponding to the correct answer.\",\n",
        "        model = MODEL_ID,\n",
        "        provider = \"openai\",\n",
        "        use_chat_completions=True,\n",
        "        extra_body={\"guided_choice\": [\"A\", \"B\", \"C\", \"D\"]},\n",
        "    )\n",
        ").with_column(\"is_correct\", col(\"result\").lstrip().rstrip() == col(\"answer\").lstrip().rstrip()).collect()\n",
        "end = time.time()\n",
        "\n",
        "print(f\"Processed {df_no_image.count_rows()} rows in {end-start} seconds\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d2EJnsabBZpj"
      },
      "outputs": [],
      "source": [
        "pass_fail_rate_no_image = df_no_image.where(col(\"is_correct\")).count_rows() / df_no_image.count_rows()\n",
        "print(f\"Pass/Fail Rate: \\n With Image: {pass_fail_rate} \\n Without Image: {pass_fail_rate_no_image} \")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# How do results change at different temperatures?\n",
        "temps = [0.0, 0.2, 0.4, 0.6, 0.8, 1.0, 1.2]\n",
        "pf_rates = []\n",
        "for temp in temps:\n",
        "    start = time.time()\n",
        "    df_no_image = df_prepped.with_column(\n",
        "        \"result\",\n",
        "        prompt(\n",
        "            [col(\"image\"), col(\"user\")],\n",
        "            system_message = \"Observe the attached image and respond to the multiple choice question with just the letter corresponding to the correct answer.\",\n",
        "            model = MODEL_ID,\n",
        "            provider = \"openai\",\n",
        "            use_chat_completions=True,\n",
        "            extra_body={\"guided_choice\": [\"A\", \"B\", \"C\", \"D\"]},\n",
        "            temperature=temp,\n",
        "        )\n",
        "    ).with_column(\"is_correct\", col(\"result\").lstrip().rstrip() == col(\"answer\").lstrip().rstrip()).collect()\n",
        "    end = time.time()\n",
        "\n",
        "    total_rows = df.count_rows\n",
        "    pass_fail_rate = df.where(col(\"is_correct\")).count_rows() / total_rows\n",
        "\n",
        "\n",
        "\n",
        "print(f\"Processed {df_no_image.count_rows()} rows in {end-start} seconds\")\n"
      ],
      "metadata": {
        "id": "l0vxHejjtt3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eALCQifQPmfp"
      },
      "source": [
        "---\n",
        "# Putting everything together: Evaluating Gemma across the AI2D Dataset\n",
        "Now that we have walked through implementing this image understanding evaluation pipeline from end to end, lets put it all together so we can take full advantage of lazy evaluation and provide opportunities for future extensibility and re-use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TObFYF7xeKFJ"
      },
      "outputs": [],
      "source": [
        "from typing import Any\n",
        "\n",
        "from openai import AsyncOpenAI\n",
        "\n",
        "import daft\n",
        "from daft import col\n",
        "from daft.functions import format, prompt\n",
        "\n",
        "\n",
        "class MultimodalStructuredOutputsEval:\n",
        "    def __init__(self, base_url: str, api_key: str):\n",
        "        self.base_url = base_url\n",
        "        self.api_key = api_key\n",
        "\n",
        "    def __call__(\n",
        "        self,\n",
        "        model_id: str,\n",
        "        dataset_uri: str,\n",
        "        sampling_params: dict[str, Any] | None = None,\n",
        "        concurrency: int = 4,\n",
        "        row_limit: int | None = None,\n",
        "        is_eager: bool = False,\n",
        "    ) -> daft.DataFrame:\n",
        "        \"\"\"Executes dataset loading, preprocessing, inference, and post-processing.\n",
        "\n",
        "        Evaluation must be run separately since it requires materialization.\n",
        "        \"\"\"\n",
        "        if is_eager:\n",
        "            # Load Dataset and Materialize\n",
        "            df = self.load_dataset(dataset_uri)\n",
        "            df = df.limit(row_limit) if row_limit else df\n",
        "            df = self._log_processing_time(df)\n",
        "\n",
        "            # Preprocess\n",
        "            df = self.preprocess(df)\n",
        "            df = self._log_processing_time(df)\n",
        "\n",
        "            # Perform Inference\n",
        "            df = self.infer(df, model_id, sampling_params)\n",
        "            df = self._log_processing_time(df)\n",
        "\n",
        "            # Post-Process\n",
        "            df = self.postprocess(df)\n",
        "            df = self._log_processing_time(df)\n",
        "        else:\n",
        "            df = self.load_dataset(dataset_uri)\n",
        "            df = self.preprocess(df)\n",
        "            df = self.infer(df, model_id, sampling_params)\n",
        "            df = self.postprocess(df)\n",
        "            df = df.limit(row_limit) if row_limit else df\n",
        "\n",
        "        return df\n",
        "\n",
        "    @staticmethod\n",
        "    def _log_processing_time(df: daft.DataFrame):\n",
        "        start = time.time()\n",
        "        df_materialized = df.collect()\n",
        "        end = time.time()\n",
        "        num_rows = df_materialized.count_rows()\n",
        "        print(f\"Processed {num_rows} rows in {end-start} sec, {num_rows/(end-start)} rows/s\")\n",
        "        return df_materialized\n",
        "\n",
        "    def load_dataset(self, uri: str) -> daft.DataFrame:\n",
        "        return daft.read_parquet(uri)\n",
        "\n",
        "    def preprocess(self, df: daft.DataFrame) -> daft.DataFrame:\n",
        "        # Convert png image byte string to base64\n",
        "        df = df.explode(col(\"images\")).with_column(\n",
        "            \"image_base64\",\n",
        "            df[\"images\"].struct.get(\"bytes\").encode(\"base64\"),\n",
        "        )\n",
        "\n",
        "        # Explode Lists of User Prompts and Assistant Answer Pairs\n",
        "        df = df.explode(col(\"texts\")).with_columns(\n",
        "            {\"user\": df[\"texts\"].struct.get(\"user\"), \"assistant\": df[\"texts\"].struct.get(\"assistant\")}\n",
        "        )\n",
        "\n",
        "        # Parse the Question/Answer Strings\n",
        "        df = df.with_columns(\n",
        "            {\n",
        "                \"question\": df[\"user\"]\n",
        "                .str.extract(r\"(?s)Question:\\s*(.*?)\\s*Choices:\")\n",
        "                .str.replace(\"Choices:\", \"\")\n",
        "                .str.replace(\"Question:\", \"\"),\n",
        "                \"choices_string\": df[\"user\"]\n",
        "                .str.extract(r\"(?s)Choices:\\s*(.*?)\\s*Answer?\\.?\")\n",
        "                .str.replace(\"Choices:\\n\", \"\")\n",
        "                .str.replace(\"Answer\", \"\"),\n",
        "                \"answer\": df[\"assistant\"].str.extract(r\"Answer:\\s*(.*)$\").str.replace(\"Answer:\", \"\"),\n",
        "            }\n",
        "        )\n",
        "        return df\n",
        "\n",
        "    def infer(\n",
        "        self,\n",
        "        df: daft.DataFrame,\n",
        "        model_id: str = \"google/gemma-3n-e4b-it\",\n",
        "        sampling_params: dict[str, Any] = {\"temperature\": 0.0},\n",
        "        concurrency: int = 4,\n",
        "        extra_body: dict[str, Any] = {\"guided_choice\": [\"A\", \"B\", \"C\", \"D\"]},\n",
        "    ) -> daft.DataFrame:\n",
        "        return df.with_column(\n",
        "            \"result\",\n",
        "            StructuredOutputsProdUDF.with_init_args(\n",
        "                base_url=self.base_url,\n",
        "                api_key=self.api_key,\n",
        "            ).with_concurrency(concurrency)(\n",
        "                model_id=model_id,\n",
        "                text_col=format(\"{} \\n {}\", col(\"question\"), col(\"choices_string\")),  # Prompt Template\n",
        "                image_col=col(\"image_base64\"),\n",
        "                sampling_params=sampling_params,\n",
        "                extra_body=extra_body,\n",
        "            ),\n",
        "        )\n",
        "\n",
        "    def postprocess(self, df: daft.DataFrame) -> daft.DataFrame:\n",
        "        df = df.with_column(\n",
        "            \"is_correct\", col(\"result\").str.lstrip().str.rstrip() == col(\"answer\").str.lstrip().str.rstrip()\n",
        "        )\n",
        "        return df\n",
        "\n",
        "    def evaluate(self, df: daft.DataFrame) -> float:\n",
        "        pass_fail_rate = df.where(col(\"is_correct\")).count_rows() / df.count_rows()\n",
        "        return pass_fail_rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A98SLoXALhxR"
      },
      "outputs": [],
      "source": [
        "# Our entire pipeline collapses into a three lines\n",
        "dataset_uri = \"hf://datasets/HuggingFaceM4/the_cauldron/ai2d/train-00000-of-00001-2ce340398c113b79.parquet\"\n",
        "pipeline = TheCauldronImageUnderstandingEvaluationPipeline(\n",
        "    api_key=OPENAI_API_KEY, base_url=OPENAI_BASE_URL, row_limit=ROW_LIMIT\n",
        ")\n",
        "df = pipeline(model_id=MODEL_ID, sampling_params={\"temperature\": 0.1}, is_eager=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mFdEC8JDMj8E"
      },
      "outputs": [],
      "source": [
        "# Materialize if not eager\n",
        "df_mat = df.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0v6x19XxNyJa"
      },
      "outputs": [],
      "source": [
        "# Print the Pass/Fail Rate\n",
        "print(f\"Pass/Fail Rate: {pipeline.evaluate(df_mat)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbzOoC62uDu1"
      },
      "source": [
        "---\n",
        "## Conclusion\n",
        "\n",
        "In this notebook we explored how to evaluate Gemma-3's image understanding using a subset from HuggingFace's TheCauldron Dataset. The AI2D subset we used is just one of a massive collection of 50 vision-language datasets that can be used for evaluating or training vision language models totaling millions of rows. You can also leverage this pipeline to evaluate model performance across sampling parameters or model variants. Please note that not all Gemma-3 series models support image inputs, and leveraging datasets outside of the TheCauldron would require different preprocessing stages.\n",
        "\n",
        "A natural next step would be to parallelize this pipeline across multiple datasets leveraging multiple gpus. In this scenario, I recommend transitioning daft's execution context to leverage Ray, a distributed compute framework.\n",
        "\n",
        "```bash\n",
        "pip install \"daft[huggingface,ray]\"\n",
        "```\n",
        "\n",
        "You can set daft's execution context to ray adding the `ray` optional dependency during installation and running the following at the top of your script.\n",
        "\n",
        "```python\n",
        "import daft\n",
        "\n",
        "daft.set_runner_ray()\n",
        "```\n",
        "\n",
        "Simply run your pipeline across each dataset uri and collect the results, Daft will orchestrate ray in the background for you."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "gqjRsfxH891x",
        "J_wIalJJs0ki"
      ],
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
