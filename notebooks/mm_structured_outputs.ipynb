{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluating Image Understanding at Scale with Structured Outputs\n",
        "\n",
        "*An end-to-end example of **Multimodal Structured Outputs** with Daft and Qwen3-VL-8B*. \n",
        "\n",
        "## Introduction\n",
        "\n",
        "In this notebook, we'll evaluate [Qwen3-VL](https://github.com/QwenLM/Qwen3-VL)'s image understanding using a multiple choice subset of HuggingFace's [The Cauldron dataset](https://huggingface.co/datasets/HuggingFaceM4/the_cauldron), a massive collection of 50 vision-language datasets. \n",
        "\n",
        "Our pipeline will:\n",
        "\n",
        "1. Run structured output inference on image+text prompts\n",
        "2. Conduct an **ablation study** (with vs. without images) to isolate image understanding\n",
        "3. Classify results into diagnostic quadrants\n",
        "4. Use **VLM-as-a-Judge** to explain failures\n",
        "\n",
        "The steps we'll take in this notebook are a simplified version of the [production-ready pipeline](https://github.com/Eventual-Inc/daft-examples/blob/main/use_cases/image_understanding_eval/eval_image_understanding.py) used to evaluate Qwen3-VL-4B on 20k rows. Check out the [blog post](https://www.daft.ai/blog/multimodal-structured-outputs-evaluating-vlm-image-understanding-at-scale) for the full results and implementation. \n",
        "\n",
        "### Table of Contents\n",
        "\n",
        "1. [Setup](#1-setup)\n",
        "2. [Data Loading](#2-data-loading)\n",
        "3. [Preprocessing](#3-preprocessing)\n",
        "4. [Structured Outputs with `prompt`](#4-structured-outputs-with-prompt)\n",
        "5. [Ablation Study](#5-ablation-study)\n",
        "6. [LLM-as-a-Judge](#6-llm-as-a-judge)\n",
        "7. [Scale with Daft Cloud](#7-scale-with-daft-cloud)\n",
        "8. [Conclusion](#8-conclusion)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Notebook vs. Production Pipeline (How this maps)\n",
        "\n",
        "This notebook is the **interactive companion** to the production evaluation pipeline in [`use_cases/image_understanding_eval/eval_image_understanding.py`](../use_cases/image_understanding_eval/eval_image_understanding.py) and the methodology described in the blog post: [Multimodal Structured Outputs: Evaluating VLM Image Understanding at Scale](https://www.daft.ai/blog/multimodal-structured-outputs-evaluating-vlm-image-understanding-at-scale).\n",
        "\n",
        "The notebook keeps `LIMIT` small so you can inspect examples, but the stages are the same:\n",
        "\n",
        "| Notebook section | Pipeline function | Purpose |\n",
        "|---|---|---|\n",
        "| Preprocessing | `preprocess()` | Extract `answer` from Cauldron text format and track config |\n",
        "| Structured Outputs (with image) | `run_inference(with_image=True)` | Predict multiple-choice letter with the image attached |\n",
        "| Ablation (no image) | `run_inference(with_image=False)` | Predict the same question *without* the image |\n",
        "| Quadrant classification | `classify_quadrants()` | Bucket behavior into Both Correct / Image Helped / Image Hurt / Both Incorrect |\n",
        "| LLM-as-a-Judge | `run_judge()` | Diagnose failure modes on the ‚ÄúImage Hurt‚Äù + ‚ÄúBoth Incorrect‚Äù subsets |\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -q daft openai numpy pillow python-dotenv ipykernel ipywidgets pydantic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "# Configuration\n",
        "MODEL_ID = \"Qwen/Qwen3-VL-8B-Instruct\"\n",
        "LIMIT = 50  # Keep low for interactive demo\n",
        "\n",
        "# HuggingFace Inference Provider (hosted Qwen3-VL endpoints)\n",
        "OPENAI_API_KEY = os.getenv(\"HF_TOKEN\")\n",
        "OPENAI_BASE_URL = \"https://router.huggingface.co/v1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import daft\n",
        "\n",
        "# Set the OpenAI-compatible provider\n",
        "daft.set_provider(\"openai\", api_key=OPENAI_API_KEY, base_url=OPENAI_BASE_URL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Loading\n",
        "\n",
        "[The Cauldron](https://huggingface.co/datasets/HuggingFaceM4/the_cauldron) is a massive collection of 50 vision-language datasets spanning:\n",
        "- Visual question answering\n",
        "- OCR & document understanding\n",
        "- Chart/figure understanding\n",
        "- Reasoning & math\n",
        "- And more...\n",
        "\n",
        "We'll start with the **AI2D** subset‚Äîscience diagrams with multiple-choice questions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "        <style>\n",
              "        .dashboard-container {\n",
              "            display: flex;\n",
              "            gap: 20px;\n",
              "            max-width: 100%;\n",
              "            height: 100%;\n",
              "        }\n",
              "        .table-container {\n",
              "            flex: 1;\n",
              "            overflow: auto;\n",
              "        }\n",
              "        .side-pane {\n",
              "            width: 35%;\n",
              "            max-height: 500px;\n",
              "            border: 1px solid;\n",
              "            border-radius: 4px;\n",
              "            padding: 15px;\n",
              "            display: none;\n",
              "            overflow: auto;\n",
              "        }\n",
              "        .side-pane.visible {\n",
              "            display: flex;\n",
              "            flex-direction: column;\n",
              "        }\n",
              "        .side-pane-header {\n",
              "            display: flex;\n",
              "            justify-content: space-between;\n",
              "            align-items: center;\n",
              "            margin-bottom: 10px;\n",
              "            padding-bottom: 10px;\n",
              "            border-bottom: 1px solid;\n",
              "        }\n",
              "        .side-pane-title {\n",
              "            font-weight: bold;\n",
              "        }\n",
              "        .close-button {\n",
              "            cursor: pointer;\n",
              "        }\n",
              "        .side-pane-content {\n",
              "            word-wrap: break-word;\n",
              "            overflow: auto;\n",
              "        }\n",
              "        .dataframe td.clickable {\n",
              "            cursor: pointer;\n",
              "            transition: background-color 0.2s;\n",
              "        }\n",
              "        .dataframe td.clickable:hover {\n",
              "            opacity: 0.8;\n",
              "        }\n",
              "        .dataframe td.clickable.selected {\n",
              "            opacity: 0.6;\n",
              "        }\n",
              "        </style>\n",
              "        <div class=\"dashboard-container\">\n",
              "            <div class=\"table-container\">\n",
              "        <div id=\"dataframe-6107f334-448b-4f42-9ef5-a6eff13f5e88\"><table class=\"dataframe\" style=\"table-layout: fixed; min-width: 100%\">\n",
              "<thead><tr><th style=\"text-wrap: nowrap; width: calc(100vw / 2); min-width: 192px; overflow: hidden; text-overflow: ellipsis; text-align:left\">images<br />List[Struct[bytes: Binary, path: String]]</th><th style=\"text-wrap: nowrap; width: calc(100vw / 2); min-width: 192px; overflow: hidden; text-overflow: ellipsis; text-align:left\">texts<br />List[Struct[user: String, assistant: String, source: String]]</th></tr></thead>\n",
              "<tbody>\n",
              "<tr><td data-row=\"0\" data-col=\"0\"><div style=\"text-align:left; width: calc(100vw / 2); min-width: 192px; max-height: 100px; overflow: hidden; text-overflow: ellipsis; word-wrap: break-word; overflow-y: auto\">[{bytes: b\"\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHD\"...,<br />path: None,<br />}]</div></td><td data-row=\"0\" data-col=\"1\"><div style=\"text-align:left; width: calc(100vw / 2); min-width: 192px; max-height: 100px; overflow: hidden; text-overflow: ellipsis; word-wrap: break-word; overflow-y: auto\">[{user: Question: What do respiration and combustion give out<br />Choices:<br />A. Oxygen<br />B. Carbon dioxide<br />C. Nitrogen<br />D. Heat<br />Answer with the letter.,<br />assistant: Answer: B,<br />source: AI2D,<br />}]</div></td></tr>\n",
              "<tr><td data-row=\"1\" data-col=\"0\"><div style=\"text-align:left; width: calc(100vw / 2); min-width: 192px; max-height: 100px; overflow: hidden; text-overflow: ellipsis; word-wrap: break-word; overflow-y: auto\">[{bytes: b\"\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHD\"...,<br />path: None,<br />}]</div></td><td data-row=\"1\" data-col=\"1\"><div style=\"text-align:left; width: calc(100vw / 2); min-width: 192px; max-height: 100px; overflow: hidden; text-overflow: ellipsis; word-wrap: break-word; overflow-y: auto\">[{user: Question: From the given food web, name any two herbivores?<br />Choices:<br />A. coyote, bobcat<br />B. dingo, jack rabbit<br />C. dingo, bobcat<br />D. roadrunner&amp;jack rabbit<br />Answer with the letter.,<br />assistant: Answer: D,<br />source: AI2D,<br />}, {user: Question: In the given food web, which are the organism that only eaten roadrunner?<br />Choices:<br />A. dingo, jack rabbit<br />B. coyote, bobcat<br />C. dingo, bobcat<br />D. snake, jack rabbit<br />Answer with the letter.,<br />assistant: Answer: B,<br />source: AI2D,<br />}, {user: Question: Name a herbivore from the given food web?<br />Choices:<br />A. cactus<br />B. kangaroo rat<br />C. snake<br />D. bobcat<br />Answer with the letter.,<br />assistant: Answer: B,<br />source: AI2D,<br />}, {user: Question: Name a producer from the given food web?<br />Choices:<br />A. bobcat<br />B. snake<br />C. road runner<br />D. barrel cactus<br />Answer with the letter.,<br />assistant: Answer: D,<br />source: AI2D,<br />}, {user: Question: Name an omnivore from the given food web?<br />Choices:<br />A. dingo<br />B. bobcat<br />C. cactus<br />D. kangaroo<br />Answer with the letter.,<br />assistant: Answer: D,<br />source: AI2D,<br />}, {user: Question: What is a predator of the roadrunner?<br />Choices:<br />A. kangaroo<br />B. coyote<br />C. dingo<br />D. cactus<br />Answer with the letter.,<br />assistant: Answer: B,<br />source: AI2D,<br />}, {user: Question: What will happen if kangroo rats goes extinct?<br />Choices:<br />A. Cactus count will decrease<br />B. Dessert plants growth will decresse<br />C. Snake population will increase<br />D. Snake population will decrease<br />Answer with the letter.,<br />assistant: Answer: D,<br />source: AI2D,<br />}, {user: Question: What would be most affected if the cactus all died?<br />Choices:<br />A. coyote<br />B. dingo<br />C. kangaroo rat<br />D. snake<br />Answer with the letter.,<br />assistant: Answer: C,<br />source: AI2D,<br />}, {user: Question: Which among the below is a producer in the food chain diagram shown?<br />Choices:<br />A. Kangaroo rat<br />B. Roadrunner<br />C. Dessert grass<br />D. Snake<br />Answer with the letter.,<br />assistant: Answer: C,<br />source: AI2D,<br />}, {user: Question: Which is a producer?<br />Choices:<br />A. Coyote<br />B. Desert Grass<br />C. Kangaroo<br />D. Dingo<br />Answer with the letter.,<br />assistant: Answer: B,<br />source: AI2D,<br />}, {user: Question: Who would suffer without kangaroo rats?<br />Choices:<br />A. Desert Grass<br />B. Snake<br />C. Cactus<br />D. Roadrunner<br />Answer with the letter.,<br />assistant: Answer: B,<br />source: AI2D,<br />}, {user: Question: desert grasses are known as<br />Choices:<br />A. consumer<br />B. herbivores<br />C. omnivores<br />D. producer<br />Answer with the letter.,<br />assistant: Answer: D,<br />source: AI2D,<br />}]</div></td></tr>\n",
              "<tr><td data-row=\"2\" data-col=\"0\"><div style=\"text-align:left; width: calc(100vw / 2); min-width: 192px; max-height: 100px; overflow: hidden; text-overflow: ellipsis; word-wrap: break-word; overflow-y: auto\">[{bytes: b\"\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHD\"...,<br />path: None,<br />}]</div></td><td data-row=\"2\" data-col=\"1\"><div style=\"text-align:left; width: calc(100vw / 2); min-width: 192px; max-height: 100px; overflow: hidden; text-overflow: ellipsis; word-wrap: break-word; overflow-y: auto\">[{user: Question: Anatomy One of a series of long curved bones occurring in 12 pairs in humans is called.<br />Choices:<br />A. diaphram<br />B. lung<br />C. none<br />D. ribs<br />Answer with the letter.,<br />assistant: Answer: D,<br />source: AI2D,<br />}]</div></td></tr>\n",
              "</tbody>\n",
              "</table></div>\n",
              "            </div>\n",
              "            <div class=\"side-pane\" id=\"side-pane-6107f334-448b-4f42-9ef5-a6eff13f5e88\">\n",
              "                <div class=\"side-pane-header\">\n",
              "                    <div class=\"side-pane-title\" id=\"side-pane-title-6107f334-448b-4f42-9ef5-a6eff13f5e88\">Cell Details</div>\n",
              "                    <button class=\"close-button\" id=\"close-button-6107f334-448b-4f42-9ef5-a6eff13f5e88\">√ó</button>\n",
              "                </div>\n",
              "                <div class=\"side-pane-content\" id=\"side-pane-content-6107f334-448b-4f42-9ef5-a6eff13f5e88\">\n",
              "                    <p style=\"font-style: italic;\">Click on a cell to view its full content</p>\n",
              "                </div>\n",
              "            </div>\n",
              "        </div>\n",
              "    \n",
              "        <script>\n",
              "        (function() {\n",
              "            const serverUrl = 'http://0.0.0.0:3238';\n",
              "            const dfId = '6107f334-448b-4f42-9ef5-a6eff13f5e88';\n",
              "            const dataframeElement = document.getElementById('dataframe-' + dfId);\n",
              "            const cells = dataframeElement ? dataframeElement.querySelectorAll('td') : [];\n",
              "            const sidePane = document.getElementById('side-pane-' + dfId);\n",
              "            const sidePaneTitle = document.getElementById('side-pane-title-' + dfId);\n",
              "            const sidePaneContent = document.getElementById('side-pane-content-' + dfId);\n",
              "            const closeButton = document.getElementById('close-button-' + dfId);\n",
              "            let selectedCell = null;\n",
              "\n",
              "            function closeSidePane(paneId) {\n",
              "                const pane = document.getElementById('side-pane-' + paneId);\n",
              "                if (pane) {\n",
              "                    pane.classList.remove('visible');\n",
              "                    if (selectedCell) {\n",
              "                        selectedCell.classList.remove('selected');\n",
              "                        selectedCell = null;\n",
              "                    }\n",
              "                }\n",
              "            }\n",
              "\n",
              "            function showSidePane(row, col, content) {\n",
              "                sidePaneTitle.textContent = 'Cell (' + row + ', ' + col + ')';\n",
              "                sidePaneContent.innerHTML = content;\n",
              "                sidePane.classList.add('visible');\n",
              "            }\n",
              "\n",
              "            function showLoadingContent() {\n",
              "                sidePaneContent.innerHTML = '<div style=\"text-align:center; padding:20px;\"><span style=\"font-style:italic\">Loading full content...</span></div>';\n",
              "            }\n",
              "\n",
              "            // Add event listener for close button\n",
              "            if (closeButton) {\n",
              "                closeButton.addEventListener('click', function() {\n",
              "                    closeSidePane(dfId);\n",
              "                });\n",
              "            }\n",
              "\n",
              "            cells.forEach((cell) => {\n",
              "                // Skip cells that do not have data-row and data-col attributes (e.g., ellipsis row)\n",
              "                const rowAttr = cell.getAttribute('data-row');\n",
              "                const colAttr = cell.getAttribute('data-col');\n",
              "                if (rowAttr === null || colAttr === null) return;\n",
              "\n",
              "                const row = parseInt(rowAttr);\n",
              "                const col = parseInt(colAttr);\n",
              "                cell.classList.add('clickable');\n",
              "\n",
              "                cell.onclick = function() {\n",
              "                    // Remove selection from previously selected cell\n",
              "                    if (selectedCell && selectedCell !== cell) {\n",
              "                        selectedCell.classList.remove('selected');\n",
              "                    }\n",
              "\n",
              "                    // Toggle selection for current cell\n",
              "                    if (selectedCell === cell) {\n",
              "                        cell.classList.remove('selected');\n",
              "                        selectedCell = null;\n",
              "                        closeSidePane(dfId);\n",
              "                        return;\n",
              "                    } else {\n",
              "                        cell.classList.add('selected');\n",
              "                        selectedCell = cell;\n",
              "                    }\n",
              "\n",
              "                    // Show the side pane immediately\n",
              "                    showSidePane(row, col, '');\n",
              "\n",
              "                    // Set a timeout to show loading content after 1 second\n",
              "                    const loadingTimeout = setTimeout(() => {\n",
              "                        showLoadingContent();\n",
              "                    }, 100);\n",
              "\n",
              "                    // Fetch the cell content\n",
              "                    fetch(serverUrl + '/api/dataframes/' + dfId + '/cell?row=' + row + '&col=' + col)\n",
              "                        .then(response => response.json())\n",
              "                        .then(data => {\n",
              "                            clearTimeout(loadingTimeout);\n",
              "                            showSidePane(row, col, data.value);\n",
              "                        })\n",
              "                        .catch(err => {\n",
              "                            clearTimeout(loadingTimeout);\n",
              "                            // Get the original cell content from the table\n",
              "                            const cell = selectedCell;\n",
              "                            if (cell) {\n",
              "                                const originalContent = cell.innerHTML;\n",
              "                                showSidePane(row, col, originalContent);\n",
              "                            }\n",
              "                        });\n",
              "                };\n",
              "            });\n",
              "        })();\n",
              "        </script>\n",
              "        \n",
              "<small>(Showing first 3 rows)</small>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "df_raw = daft.read_huggingface(\"HuggingFaceM4/the_cauldron/ai2d\").limit(LIMIT).collect()\n",
        "df_raw.show(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Preprocessing\n",
        "\n",
        "We need to:\n",
        "1. Decode images into Daft's Image type\n",
        "2. Extract the question, choices, and correct answer from the text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from daft import col\n",
        "from daft.functions import unnest\n",
        "\n",
        "# Decode images\n",
        "df_img = df_raw.explode(col(\"images\"))\n",
        "df_img = df_img.with_column(\"image\", col(\"images\")[\"bytes\"].decode_image())\n",
        "\n",
        "# Extract text fields (user question, assistant answer)\n",
        "df_text = df_img.explode(col(\"texts\")).select(unnest(col(\"texts\")), \"image\")\n",
        "\n",
        "# Parse the answer letter from \"Answer: C\" format\n",
        "df_prep = df_text.with_column(\n",
        "    \"answer\", \n",
        "    col(\"assistant\").regexp_replace(\"Answer: \", \"\").lstrip().rstrip()\n",
        ").collect()\n",
        "\n",
        "df_prep.show(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Structured Outputs with `prompt`\n",
        "\n",
        "Daft's `prompt` function scales OpenAI-compatible calls across dataframes. We'll use a Pydantic model to enforce structured output.\n",
        "\n",
        "For more info: [API docs](https://docs.daft.ai/en/stable/api/functions/prompt/) | [User Guide](https://docs.daft.ai/en/stable/ai-functions/prompt/)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from daft.functions import prompt\n",
        "from pydantic import BaseModel, Field\n",
        "import time\n",
        "\n",
        "# Deterministic inference params (matches the production pipeline defaults)\n",
        "PARAMS = {\"temperature\": 0.0, \"max_tokens\": 2}\n",
        "\n",
        "class ChoiceResponse(BaseModel):\n",
        "    \"\"\"Structured output for multiple choice answers.\"\"\"\n",
        "    choice: str = Field(\n",
        "        ..., description=\"The letter of the correct choice (e.g., A, B, C, D)\"\n",
        "    )\n",
        "\n",
        "start = time.time()\n",
        "df_results = df_prep.with_column(\n",
        "    \"result\",\n",
        "    prompt(\n",
        "        messages=[col(\"image\"), col(\"user\")],\n",
        "        model=MODEL_ID,\n",
        "        use_chat_completions=True,\n",
        "        return_format=ChoiceResponse,\n",
        "        **PARAMS,\n",
        "    )\n",
        ").limit(LIMIT).collect()\n",
        "elapsed = time.time() - start\n",
        "\n",
        "print(f\"Processed {df_results.count_rows()} rows in {elapsed:.1f} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate correctness\n",
        "df_eval = df_results.with_column(\n",
        "    \"is_correct\", \n",
        "    col(\"result\")[\"choice\"].lstrip().rstrip() == col(\"answer\").lstrip().rstrip()\n",
        ")\n",
        "\n",
        "accuracy = df_eval.where(col(\"is_correct\")).count_rows() / df_eval.count_rows()\n",
        "print(f\"Accuracy (with image): {accuracy:.1%}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let's look at some results\n",
        "df_eval.select(\"user\", \"image\", \"answer\", col(\"result\")[\"choice\"].alias(\"predicted\"), \"is_correct\").show(5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Ablation Study\n",
        "\n",
        "A simple accuracy score tells us *how often* the model is correct, but not *why*. To understand the contribution of image understanding, we'll conduct an **ablation study**‚Äîrunning the same prompts without images.\n",
        "\n",
        "This lets us classify each example into four quadrants:\n",
        "\n",
        "| Quadrant | With Image | Without Image | Interpretation |\n",
        "|----------|------------|---------------|----------------|\n",
        "| **Both Correct** | ‚úì | ‚úì | Question may be solvable from text alone |\n",
        "| **Image Helped** | ‚úì | ‚úó | True image understanding |\n",
        "| **Image Hurt** | ‚úó | ‚úì | Visual confusion |\n",
        "| **Both Incorrect** | ‚úó | ‚úó | Hard question or model limitation |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run without images\n",
        "SYSTEM_PROMPT_NO_IMAGE = \"Respond to the multiple choice question with just the letter corresponding to the correct answer.\"\n",
        "\n",
        "start = time.time()\n",
        "df_ablation = df_eval.with_column(\n",
        "    \"result_no_image\",\n",
        "    prompt(\n",
        "        messages=col(\"user\"),\n",
        "        system_message=SYSTEM_PROMPT_NO_IMAGE,\n",
        "        model=MODEL_ID,\n",
        "        use_chat_completions=True,\n",
        "        return_format=ChoiceResponse,\n",
        "        **PARAMS,\n",
        "    )\n",
        ").with_column(\n",
        "    \"is_correct_no_image\",\n",
        "    col(\"result_no_image\")[\"choice\"].lstrip().rstrip() == col(\"answer\").lstrip().rstrip()\n",
        ").collect()\n",
        "elapsed = time.time() - start\n",
        "\n",
        "print(f\"Processed {df_ablation.count_rows()} rows in {elapsed:.1f} seconds\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare accuracy\n",
        "accuracy_no_image = df_ablation.where(col(\"is_correct_no_image\")).count_rows() / df_ablation.count_rows()\n",
        "\n",
        "print(f\"Accuracy with image:    {accuracy:.1%}\")\n",
        "print(f\"Accuracy without image: {accuracy_no_image:.1%}\")\n",
        "print(f\"Delta:                  {accuracy - accuracy_no_image:+.1%}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from daft.functions import when, monotonically_increasing_id\n",
        "\n",
        "# Classify into quadrants\n",
        "df_classified = df_ablation.with_column(\n",
        "    \"id\", monotonically_increasing_id()\n",
        ").with_column(\n",
        "    \"quadrant\",\n",
        "    when((col(\"is_correct\") == True) & (col(\"is_correct_no_image\") == True), \"Both Correct\")\n",
        "    .when((col(\"is_correct\") == True) & (col(\"is_correct_no_image\") == False), \"Image Helped\")\n",
        "    .when((col(\"is_correct\") == False) & (col(\"is_correct_no_image\") == True), \"Image Hurt\")\n",
        "    .otherwise(\"Both Incorrect\")\n",
        ")\n",
        "\n",
        "# Show distribution\n",
        "df_classified.groupby(\"quadrant\").count().select(\"quadrant\", col(\"id\").alias(\"count\")).show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inspect cases where the image helped\n",
        "df_classified.where(col(\"quadrant\") == \"Image Helped\").select(\n",
        "    \"user\", \"image\", \"answer\", \n",
        "    col(\"result\")[\"choice\"].alias(\"with_image\"),\n",
        "    col(\"result_no_image\")[\"choice\"].alias(\"without_image\")\n",
        ").show(3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inspect cases where the image hurt\n",
        "df_classified.where(col(\"quadrant\") == \"Image Hurt\").select(\n",
        "    \"user\", \"image\", \"answer\",\n",
        "    col(\"result\")[\"choice\"].alias(\"with_image\"),\n",
        "    col(\"result_no_image\")[\"choice\"].alias(\"without_image\")\n",
        ").show(3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show breakdown by quadrant with percentages\n",
        "total_count = df_classified.count_rows()\n",
        "\n",
        "df_results = df_classified.groupby(\"quadrant\").count().select(\n",
        "    \"quadrant\",\n",
        "    col(\"id\").alias(\"count\")\n",
        ").with_column(\n",
        "    \"percentage\",\n",
        "    (col(\"count\") / daft.lit(total_count) * 100)\n",
        ").collect()\n",
        "\n",
        "df_results.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. LLM-as-a-Judge\n",
        "\n",
        "We can use a LLM to judge the correctness of the model's structured outputs. Here we'll use a simple prompt to judge whether the model's choice is correct and to diagnose the failure mode."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "JUDGE_SYSTEM_PROMPT = \"\"\"\n",
        "You are an impartial judge reviewing the results of a textbook academic questions multiple choice benchmark.\n",
        "Inspect the attached image and provide high-signal feedback on why the model chose its answer.\n",
        "First, reason about the model's answer with the image and the model's answer without the image.\n",
        "Second, develop a hypothesis for why the model made the choice it did. \n",
        "Third, attribute the failure to a 'question' issue or an 'image' understanding issue.\n",
        "Finally, assign whether the model's answer with the image is correct and whether the model's answer without the image is correct.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class JudgeResponse(BaseModel):\n",
        "    \"\"\"Structured diagnostic feedback from the VLM judge.\"\"\"\n",
        "\n",
        "    reasoning: str = Field(\n",
        "        ..., description=\"Why did the model choose the answer it did?\"\n",
        "    )\n",
        "    hypothesis: str = Field(\n",
        "        ..., description=\"What caused the divergence from the correct answer?\"\n",
        "    )\n",
        "    attribution: str = Field(\n",
        "        ...,\n",
        "        description=\"Was this a 'question' issue or an 'image' understanding issue or 'other'?\",\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from daft.functions import format\n",
        "\n",
        "# Build a judge prompt \n",
        "judge_template = format(\n",
        "    \"\"\"Given the image attached and the multiple choice question of <question>{}</question>,\n",
        "The model chose the following prediction <model_answer>{}</model_answer> and without the image, the model chose the following prediction <no_image_model_answer>{}</no_image_model_answer>, but the correct answer is <correct_answer>{}</correct_answer>.\n",
        "\n",
        "Provide diagnostic feedback.\n",
        "\"\"\",\n",
        "    col(\"user\"),\n",
        "    col(\"result\")[\"choice\"],\n",
        "    col(\"result_no_image\")[\"choice\"],\n",
        "    col(\"answer\"),\n",
        ")\n",
        "\n",
        "# Run judge on the same failure quadrants as the production pipeline\n",
        "df_failures = df_classified.where(\n",
        "    (col(\"quadrant\") == \"Image Hurt\") | (col(\"quadrant\") == \"Both Incorrect\")\n",
        ")\n",
        "\n",
        "# Judge needs more tokens than the multiple-choice inference passes.\n",
        "JUDGE_PARAMS = {\"temperature\": 0.0, \"max_tokens\": 512}\n",
        "\n",
        "df_judged = df_failures.with_column(\n",
        "    \"judge_response\",\n",
        "    prompt(\n",
        "        messages=[col(\"image\"), judge_template],\n",
        "        system_message=JUDGE_SYSTEM_PROMPT,\n",
        "        model=MODEL_ID,\n",
        "        use_chat_completions=True,\n",
        "        return_format=JudgeResponse,\n",
        "        **JUDGE_PARAMS,\n",
        "    ),\n",
        ").collect()\n",
        "\n",
        "print(f\"Judged {df_judged.count_rows()} failure rows\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Interpreting Judge Feedback\n",
        "\n",
        "The Judge is most useful on:\n",
        "- **Image Hurt**: the model was correct *without* the image but incorrect *with* the image (the image introduced confusion).\n",
        "- **Both Incorrect**: the model missed in both conditions (hard question, ambiguity, or capability gap).\n",
        "\n",
        "Use the judge‚Äôs `attribution` signal to quickly separate **question issues** (ambiguous prompt/choices) from **image understanding issues** (missed labels, small text, visual ambiguity). For more on these failure modes and what we observed at scale, see the accompanying blog post: [Multimodal Structured Outputs: Evaluating VLM Image Understanding at Scale](https://www.daft.ai/blog/multimodal-structured-outputs-evaluating-vlm-image-understanding-at-scale).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from daft.functions import unnest\n",
        "\n",
        "# Inspect a few judged failures (interactive)\n",
        "df_judged.select(\n",
        "    \"quadrant\",\n",
        "    \"user\",\n",
        "    \"image\",\n",
        "    \"answer\",\n",
        "    col(\"result\")[\"choice\"].alias(\"with_image\"),\n",
        "    col(\"result_no_image\")[\"choice\"].alias(\"without_image\"),\n",
        "    unnest(col(\"judge_response\")),\n",
        ").show(3)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sanity checks: did we run every stage?\n",
        "\n",
        "# Accuracies\n",
        "print(f\"Accuracy (with image):    {accuracy:.1%}\")\n",
        "print(f\"Accuracy (without image): {accuracy_no_image:.1%}\")\n",
        "print(f\"Delta:                   {accuracy - accuracy_no_image:+.1%}\")\n",
        "\n",
        "# Quadrant distribution\n",
        "df_classified.groupby(\"quadrant\").count().show()\n",
        "\n",
        "# Judge coverage\n",
        "print(f\"Judge rows: {df_judged.count_rows()}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Scale with Daft Cloud\n",
        "\n",
        "**Everything above runs locally on 50 rows.**\n",
        "\n",
        "But The Cauldron contains **millions of rows across 50 subsets**. To run this evaluation at scale with strong consistent performance we can scale on [Daft Cloud](https://daft.ai/cloud). The python script version of this notebook is available in the [daft-examples](https://github.com/Eventual-Inc/daft-examples) repo in the [use_cases/image_understanding_eval](https://github.com/Eventual-Inc/daft-examples/tree/main/use_cases/image_understanding_eval) directory.\n",
        "\n",
        "üëâ [**Sign up for early access**](https://daft.ai/cloud) | [**Book a demo**](https://www.daft.ai/demo) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Conclusion\n",
        "\n",
        "In this notebook, we built a small pipeline to evaluate Qwen3-VL's image understanding:\n",
        "\n",
        "1. **Structured Outputs**: Used Pydantic models to enforce consistent responses\n",
        "2. **Ablation Study**: Isolated image understanding from general reasoning\n",
        "3. **Quadrant Analysis**: Classified results into actionable categories\n",
        "4. **LLM-as-a-Judge**: Diagnosed failures on the most informative subsets (\"Image Hurt\" + \"Both Incorrect\")\n",
        "\n",
        "---\n",
        "\n",
        "### Resources\n",
        "\n",
        "- [Daft Documentation](https://docs.daft.ai)\n",
        "- [Daft Cloud](https://daft.ai/cloud)\n",
        "- [The Cauldron Dataset](https://huggingface.co/datasets/HuggingFaceM4/the_cauldron)\n",
        "- [Qwen3-VL](https://github.com/QwenLM/Qwen3-VL)\n",
        "\n",
        "**Canonical References:**\n",
        "- [Getting Structured LLM Output (DeepLearning.ai)](https://learn.deeplearning.ai/courses/getting-structured-llm-output/information)\n",
        "- [Judging LLM-as-a-Judge (NeurIPS 2023)](https://arxiv.org/abs/2306.05685)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
