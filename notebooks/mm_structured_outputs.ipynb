{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluating Image Understanding at Scale with Structured Outputs\n",
        "\n",
        "*An end-to-end example of **Multimodal Structured Outputs** with Daft and Qwen3-VL-8B*\n",
        "\n",
        "\n",
        "\n",
        "## Introduction\n",
        "\n",
        "**Structured Outputs** refers to a family of features that enables language models to respond in a constrained format. While language models continue to improve and demonstrate emergent abilities, their unpredictable nature makes them difficult to integrate with traditional software systems. Most real-world AI use cases leverage structured outputs to some extent, whether that be to execute tool calls or adhere to Pydantic Models.\n",
        "\n",
        "The underlying technology is called **guided decoding**, using logits to control model output by adjusting token probabilities to enforce constraints. This can be done through logit biases, Finite State Machines (FSM), or other techniques that interact with the model's probability distribution.\n",
        "\n",
        "Structured Outputs generally supports 5 constraint strategies:\n",
        "- **Basic Python Types**: `int`, `float`, `bool`...\n",
        "- **Multiple Choices**: using `Literal` or `Enum`\n",
        "- **JSON Schemas**: using Pydantic models or dataclasses\n",
        "- **Regex**\n",
        "- **Context-free Grammars**\n",
        "\n",
        "### What We'll Build\n",
        "\n",
        "We'll evaluate [Qwen3-VL](https://github.com/QwenLM/Qwen3-VL)'s image understanding using a multiple choice subset of HuggingFace's [The Cauldron dataset](https://huggingface.co/datasets/HuggingFaceM4/the_cauldron), a massive collection of 50 vision-language datasets. \n",
        "\n",
        "Our pipeline will:\n",
        "\n",
        "1. Run structured output inference on image+text prompts\n",
        "2. Conduct an **ablation study** (with vs. without images) to isolate image understanding\n",
        "3. Classify results into diagnostic quadrants\n",
        "4. Use **VLM-as-a-Judge** to explain failures\n",
        "\n",
        "\n",
        "### Table of Contents\n",
        "\n",
        "1. [Setup](#1-setup)\n",
        "2. [Data Loading](#2-data-loading)\n",
        "3. [Preprocessing](#3-preprocessing)\n",
        "4. [Structured Outputs with `prompt`](#4-structured-outputs-with-prompt)\n",
        "5. [Ablation Study](#5-ablation-study)\n",
        "6. [Scale with Daft Cloud](#6-scale-with-daft-cloud)\n",
        "7. [Conclusion](#7-conclusion)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -q daft openai numpy pillow python-dotenv ipykernel ipywidgets pydantic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "# Configuration\n",
        "MODEL_ID = \"Qwen/Qwen3-VL-8B-Instruct\"\n",
        "LIMIT = 50  # Keep low for interactive demo\n",
        "\n",
        "# HuggingFace Inference Provider (hosted Qwen3-VL endpoints)\n",
        "OPENAI_API_KEY = os.getenv(\"HF_TOKEN\")\n",
        "OPENAI_BASE_URL = \"https://router.huggingface.co/v1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import daft\n",
        "\n",
        "# Set the OpenAI-compatible provider\n",
        "daft.set_provider(\"openai\", api_key=OPENAI_API_KEY, base_url=OPENAI_BASE_URL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Loading\n",
        "\n",
        "[The Cauldron](https://huggingface.co/datasets/HuggingFaceM4/the_cauldron) is a massive collection of 50 vision-language datasets spanning:\n",
        "- Visual question answering\n",
        "- OCR & document understanding\n",
        "- Chart/figure understanding\n",
        "- Reasoning & math\n",
        "- And more...\n",
        "\n",
        "We'll start with the **AI2D** subsetâ€”science diagrams with multiple-choice questions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_raw = daft.read_huggingface(\"HuggingFaceM4/the_cauldron/ai2d\").limit(LIMIT).collect()\n",
        "df_raw.show(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Preprocessing\n",
        "\n",
        "We need to:\n",
        "1. Decode images into Daft's Image type\n",
        "2. Extract the question, choices, and correct answer from the text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from daft import col\n",
        "from daft.functions import unnest\n",
        "\n",
        "# Decode images\n",
        "df_img = df_raw.explode(col(\"images\"))\n",
        "df_img = df_img.with_column(\"image\", col(\"images\")[\"bytes\"].decode_image())\n",
        "\n",
        "# Extract text fields (user question, assistant answer)\n",
        "df_text = df_img.explode(col(\"texts\")).select(unnest(col(\"texts\")), \"image\")\n",
        "\n",
        "# Parse the answer letter from \"Answer: C\" format\n",
        "df_prep = df_text.with_column(\n",
        "    \"answer\", \n",
        "    col(\"assistant\").regexp_replace(\"Answer: \", \"\").lstrip().rstrip()\n",
        ").collect()\n",
        "\n",
        "df_prep.show(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Structured Outputs with `prompt`\n",
        "\n",
        "Daft's `prompt` function scales OpenAI-compatible calls across dataframes. We'll use a Pydantic model to enforce structured output.\n",
        "\n",
        "For more info: [API docs](https://docs.daft.ai/en/stable/api/functions/prompt/) | [User Guide](https://docs.daft.ai/en/stable/ai-functions/prompt/)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from daft.functions import prompt\n",
        "from pydantic import BaseModel, Field\n",
        "import time\n",
        "\n",
        "class ChoiceResponse(BaseModel):\n",
        "    \"\"\"Structured output for multiple choice answers.\"\"\"\n",
        "    choice: str  = Field(..., description=\"The letter of the correct choice (e.g., A, B, C, D)\")\n",
        "\n",
        "start = time.time()\n",
        "df_results = df_prep.with_column(\n",
        "    \"result\",\n",
        "    prompt(\n",
        "        messages=[col(\"image\"), col(\"user\")],\n",
        "        model=MODEL_ID,\n",
        "        use_chat_completions=True,\n",
        "        return_format=ChoiceResponse,\n",
        "    )\n",
        ").limit(LIMIT).collect()\n",
        "elapsed = time.time() - start\n",
        "\n",
        "print(f\"Processed {df_results.count_rows()} rows in {elapsed:.1f} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate correctness\n",
        "df_eval = df_results.with_column(\n",
        "    \"is_correct\", \n",
        "    col(\"result\")[\"choice\"].lstrip().rstrip() == col(\"answer\").lstrip().rstrip()\n",
        ")\n",
        "\n",
        "accuracy = df_eval.where(col(\"is_correct\")).count_rows() / df_eval.count_rows()\n",
        "print(f\"Accuracy (with image): {accuracy:.1%}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let's look at some results\n",
        "df_eval.select(\"user\", \"image\", \"answer\", col(\"result\")[\"choice\"].alias(\"predicted\"), \"is_correct\").show(5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Ablation Study\n",
        "\n",
        "A simple accuracy score tells us *how often* the model is correct, but not *why*. To understand the contribution of image understanding, we'll conduct an **ablation study**â€”running the same prompts without images.\n",
        "\n",
        "This lets us classify each example into four quadrants:\n",
        "\n",
        "| Quadrant | With Image | Without Image | Interpretation |\n",
        "|----------|------------|---------------|----------------|\n",
        "| **Both Correct** | âœ“ | âœ“ | Question may be solvable from text alone |\n",
        "| **Image Helped** | âœ“ | âœ— | True image understanding |\n",
        "| **Image Hurt** | âœ— | âœ“ | Visual confusion |\n",
        "| **Both Incorrect** | âœ— | âœ— | Hard question or model limitation |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run without images\n",
        "SYSTEM_PROMPT_NO_IMAGE = \"Respond to the multiple choice question with just the letter corresponding to the correct answer.\"\n",
        "\n",
        "start = time.time()\n",
        "df_ablation = df_eval.with_column(\n",
        "    \"result_no_image\",\n",
        "    prompt(\n",
        "        messages=col(\"user\"),\n",
        "        system_message=SYSTEM_PROMPT_NO_IMAGE,\n",
        "        model=MODEL_ID,\n",
        "        use_chat_completions=True,\n",
        "        return_format=ChoiceResponse,\n",
        "    )\n",
        ").with_column(\n",
        "    \"is_correct_no_image\",\n",
        "    col(\"result_no_image\")[\"choice\"].lstrip().rstrip() == col(\"answer\").lstrip().rstrip()\n",
        ").collect()\n",
        "elapsed = time.time() - start\n",
        "\n",
        "print(f\"Processed {df_ablation.count_rows()} rows in {elapsed:.1f} seconds\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare accuracy\n",
        "accuracy_no_image = df_ablation.where(col(\"is_correct_no_image\")).count_rows() / df_ablation.count_rows()\n",
        "\n",
        "print(f\"Accuracy with image:    {accuracy:.1%}\")\n",
        "print(f\"Accuracy without image: {accuracy_no_image:.1%}\")\n",
        "print(f\"Delta:                  {accuracy - accuracy_no_image:+.1%}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from daft.functions import when, monotonically_increasing_id\n",
        "\n",
        "# Classify into quadrants\n",
        "df_classified = df_ablation.with_column(\n",
        "    \"id\", monotonically_increasing_id()\n",
        ").with_column(\n",
        "    \"quadrant\",\n",
        "    when((col(\"is_correct\") == True) & (col(\"is_correct_no_image\") == True), \"Both Correct\")\n",
        "    .when((col(\"is_correct\") == True) & (col(\"is_correct_no_image\") == False), \"Image Helped\")\n",
        "    .when((col(\"is_correct\") == False) & (col(\"is_correct_no_image\") == True), \"Image Hurt\")\n",
        "    .otherwise(\"Both Incorrect\")\n",
        ")\n",
        "\n",
        "# Show distribution\n",
        "df_classified.groupby(\"quadrant\").count().select(\"quadrant\", col(\"id\").alias(\"count\")).show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inspect cases where the image helped\n",
        "df_classified.where(col(\"quadrant\") == \"Image Helped\").select(\n",
        "    \"user\", \"image\", \"answer\", \n",
        "    col(\"result\")[\"choice\"].alias(\"with_image\"),\n",
        "    col(\"result_no_image\")[\"choice\"].alias(\"without_image\")\n",
        ").show(3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inspect cases where the image hurt\n",
        "df_classified.where(col(\"quadrant\") == \"Image Hurt\").select(\n",
        "    \"user\", \"image\", \"answer\",\n",
        "    col(\"result\")[\"choice\"].alias(\"with_image\"),\n",
        "    col(\"result_no_image\")[\"choice\"].alias(\"without_image\")\n",
        ").show(3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show breakdown by quadrant with percentages\n",
        "total_count = df_classified.count_rows()\n",
        "\n",
        "df_results = df_classified.groupby(\"quadrant\").count().select(\n",
        "    \"quadrant\",\n",
        "    col(\"id\").alias(\"count\")\n",
        ").with_column(\n",
        "    \"percentage\",\n",
        "    (col(\"count\") / daft.lit(total_count) * 100)\n",
        ").collect()\n",
        "\n",
        "df_results.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Scale with Daft Cloud\n",
        "\n",
        "**Everything above runs locally on 50 rows.**\n",
        "\n",
        "But The Cauldron contains **millions of rows across 50 subsets**. To run this evaluation at scale with strong consistent performance we can scale on [Daft Cloud](https://daft.ai/cloud). The python script version of this notebook is available in the [daft-examples](https://github.com/Eventual-Inc/daft-examples) repo in the [use_cases/image_understanding_eval](https://github.com/Eventual-Inc/daft-examples/tree/main/use_cases/image_understanding_eval) directory.\n",
        "\n",
        "ðŸ‘‰ [**Sign up for early access**](https://daft.ai/cloud) | [**Book a demo**](https://www.daft.ai/demo) \n",
        "\n",
        "---\n",
        "\n",
        "### Take this one step further with LLM-as-a-judge\n",
        "For a daft cloud ready-to-run script with a bonus LLM-as-a-judge section, check out [`eval_image_understanding.py`](https://github.com/Eventual-Inc/daft-examples/blob/main/use_cases/image_understanding_eval/eval_image_understanding.py) in the [daft-examples](https://github.com/Eventual-Inc/daft-examples) repo. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Conclusion\n",
        "\n",
        "In this notebook, we built a small pipeline to evaluate Qwen3-VL's image understanding:\n",
        "\n",
        "1. **Structured Outputs**: Used Pydantic models to enforce consistent responses\n",
        "2. **Ablation Study**: Isolated image understanding from general reasoning\n",
        "3. **Quadrant Analysis**: Classified results into actionable categories\n",
        "### Next Steps\n",
        "\n",
        "**Multi-Dataset Evaluation**: Extend across all 50 Cauldron subsets\n",
        "```python\n",
        "subsets = [\"ai2d\", \"chartqa\", \"docvqa\", \"infographicvqa\", ...]\n",
        "for subset in subsets:\n",
        "    df = run_full_pipeline(subset, MODEL_ID)\n",
        "    df.write_parquet(f\"results/{subset}.parquet\")\n",
        "```\n",
        "\n",
        "**Experiment Tracking**: Wire judge feedback into MLflow or W&B to track improvements over time.\n",
        "\n",
        "**RLVR Training**: Use the `is_correct` signal and judge attributions for reinforcement learning with verifiable rewards.\n",
        "\n",
        "---\n",
        "\n",
        "### Resources\n",
        "\n",
        "- [Daft Documentation](https://docs.daft.ai)\n",
        "- [Daft Cloud](https://daft.ai/cloud)\n",
        "- [The Cauldron Dataset](https://huggingface.co/datasets/HuggingFaceM4/the_cauldron)\n",
        "- [Qwen3-VL](https://github.com/QwenLM/Qwen3-VL)\n",
        "\n",
        "**Canonical References:**\n",
        "- [Getting Structured LLM Output (DeepLearning.ai)](https://learn.deeplearning.ai/courses/getting-structured-llm-output/information)\n",
        "- [Judging LLM-as-a-Judge (NeurIPS 2023)](https://arxiv.org/abs/2306.05685)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
