{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8n0CLwwb5dZ"
      },
      "source": [
        "# Evaluating Image Understanding at Scale with Structured Outputs and LLM-as-a-Judge Feedback\n",
        "\n",
        "*An end-to-end example of **Multimodal Structured Outputs** with Daft, vLLM, and Qwen3-VL-8B-Instruct*\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/everettVT/daft-examples-1/blob/mm-structured-outptus/notebooks/mm_structured_outputs.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "\n",
        "## Introduction\n",
        "\n",
        "**Structured Outputs** refers to a family of features that enables language models to respond in a constrained format. While language models continue to improve and demonstrate emergent abilities, their unpredictable nature make them difficult to integrate with traditional software systems. Most real-world AI use cases leverage structured outputs to some extent, whether that be to execute tool calls or adhere to Pydantic Models.\n",
        "\n",
        "As a core primitive of almost every LM workload, here are a few canonical references for structured outputs that are worth saving:\n",
        "\n",
        "- [**Getting Structured LLM Output** from dottxt founders Will Kurt & Cameron Pfiffer (DeepLearning.ai Course)](https://learn.deeplearning.ai/courses/getting-structured-llm-output/information)\n",
        "- [**Coding for Structured Generation with LLMs** by Will Kurt](https://blog.dottxt.ai/coding-for-structured-generation.html)\n",
        "- [**Structured Decoding in vLLM: A Gentle Introduction** by Aaron Pham](https://www.bentoml.com/blog/structured-decoding-in-vllm-a-gentle-introduction#user-content-fn-7)\n",
        "\n",
        "### What is Structured Outputs?\n",
        "\n",
        "The underlying technology that makes structured outputs is called guided decoding. Guided decoding uses logits to control the output of a language model by adjusting the probabilities of the next possible tokens to enforce constraints or guide the generation process. This can be done through various methods, such as applying a logit bias to penalize or promote specific tokens, filtering invalid tokens based on rules like a Finite State Machine (FSM), or by using more advanced techniques to interact with the model's internal probability distribution.\n",
        "\n",
        "Structured Outputs consists of 5 strategies that define the desired output type:\n",
        "\n",
        "- Basic Python Types: `int`, `float`, `bool`...\n",
        "- Multiple Choices: using `Literal` or `Enum`\n",
        "- JSON Schemas: using Pydantic models or dataclasses\n",
        "- Regex\n",
        "- Context-free Grammars\n",
        "\n",
        "While there are tremendous number of examples in pure python, few tutorials exist that demonstrate structured outputs within a large-scale processing context. Even fewer, if any, examples exist that demonstrate how to run batch structured outputs with multimodal data on your own inference server. Here, we will walk you through the entire process, using your own OpenAI-compatible server using [vLLM](https://docs.vllm.ai/en/v0.6.3.post1/serving/openai_compatible_server.html).\n",
        "\n",
        "### What is an LLM-as-a-Judge?\n",
        "\n",
        "**LLM-as-a-Judge** is a framework where a language model evaluates the outputs of other AI systems—providing qualitative assessments beyond simple accuracy metrics. Rather than relying solely on human evaluation (expensive, slow, inconsistent) or traditional metrics like BLEU/ROUGE (surface-level, miss semantic nuance), LLM judges can assess relevance, coherence, factual accuracy, and instruction adherence at scale.\n",
        "\n",
        "The approach was formalized in the [seminal paper introducing MT-Bench and Chatbot Arena](https://arxiv.org/abs/2306.05685), which demonstrated that strong LLMs like GPT-4 can achieve ~80% agreement with human preferences—comparable to inter-annotator agreement between humans themselves.\n",
        "\n",
        "Three common evaluation methods exist within the LLM-as-a-Judge framework:\n",
        "\n",
        "1. **Pairwise Comparison**: The judge evaluates two responses and determines which is superior\n",
        "2. **Single Answer Grading**: The judge assigns a score to a single response based on predefined criteria\n",
        "3. **Reference-Guided Grading**: The judge compares a response against a known correct answer\n",
        "\n",
        "In practice, these methods can be extended beyond simple grading to provide **diagnostic feedback**—analyzing *why* a model succeeded or failed, not just *whether* it did. This paradigm also extends naturally to vision-language models (**VLM-as-a-Judge**), enabling evaluation of multimodal outputs involving both text and images. In this notebook, we combine Reference-Guided Grading with diagnostic failure attribution to analyze our image understanding pipeline.\n",
        "\n",
        "Some core canonical references for LLM-as-a-Judge include:\n",
        "\n",
        "- [**Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena** by Zheng et al. (NeurIPS 2023)](https://arxiv.org/abs/2306.05685)\n",
        "- [**Using an LLM-as-a-Judge** from the Cloud Security Alliance](https://cloudsecurityalliance.org/articles/using-an-llm-as-a-judge)\n",
        "- [**LLM-as-a-Judge** on Wikipedia](https://en.wikipedia.org/wiki/LLM-as-a-Judge)\n",
        "\n",
        "### Why This Matters\n",
        "\n",
        "Large scale multimodal structured outputs is a real world use-case that every enterprise team faces when attempting to work with massive amounts of internal/private data. These teams face significant hurdles with traditional tooling, especially for cutting-edge uses cases like batch tool calls for background agents or reinforcement learning with verifiable rewards.\n",
        "\n",
        "Daft's unified multimodal data processing engine is purpose built to support workloads like this and is rapidly becoming the default engine of choice for teams deploying frontier AI solutions in production.\n",
        "\n",
        "In this notebook, we will leverage Daft to evaluate the image understanding accuracy of [Qwen3 VL 8b](https://github.com/QwenLM/Qwen3-VL) using HugginFace's [the_cauldron dataset](https://huggingface.co/datasets/HuggingFaceM4/the_cauldron). By the end of this notebook, you will be ready to implement your own distributed batch structured outputs pipeline with a copy-paste script you can use in your own environment.\n",
        "\n",
        "---\n",
        "\n",
        "### Table of Contents\n",
        "\n",
        "1. [Setup](#1-setup)\n",
        "2. [Inference Configuration and Setup](#2-inference-configuration-and-setup)\n",
        "3. [Sanity Checks](#3-sanity-check-openai-client-requests)\n",
        "4. [Data Loading and Preprocessing](#4-data-loading-and-preprocessing)\n",
        "5. [Multimodal Structured Outputs with `prompt`](#5-multimodal-structured-outputs-with-the-prompt-function)\n",
        "6. [Ablation Study](#6-analysis)\n",
        "7. [Accuracy Analysis and Comparison](#7-analyzing-the-results)\n",
        "8. [LLM-as-a-Judge Evaluation](#8-llm-as-a-judge-evaluation)\n",
        "9. [Putting it all together](#9-putting-it-all-together)\n",
        "10. [Conclusion](#conclusion)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8P5OPUCtIkO"
      },
      "source": [
        "## 1. Setup\n",
        "\n",
        "### Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kxDFf_3iZyzq"
      },
      "outputs": [],
      "source": [
        "!pip install -q \"daft>=0.6.14\" openai numpy pillow ipykernel ipywidgets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G311EzbhZyzq"
      },
      "source": [
        "### Configure Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N4VKXxT1Zyzq"
      },
      "outputs": [],
      "source": [
        "MODEL_ID = \"qwen/qwen3-vl-8b-instruct\"        # vLLM & OpenRouter\n",
        "DATASET_URI = \"HuggingFaceM4/the_cauldron\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtIxKYwIrjG7"
      },
      "source": [
        "### Inference Configuration and Setup\n",
        "\n",
        "**Using an OpenAI compatible Provider (OpenRouter)**\n",
        "\n",
        "[OpenRouter](https://openrouter.ai/models?max_price=0.5&order=top-weekly) has model endpoints for [Qwen/Qwen3-VL-8B-Instruct](https://huggingface.co/Qwen/Qwen3-VL-8B-Instruct)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YAvrx9-oZyzr"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv() # load your environment variables from .env file\n",
        "\n",
        "OPENAI_API_KEY = os.environ.get(\"OPENROUTER_API_KEY\")\n",
        "OPENAI_BASE_URL = \"https://openrouter.ai/api/v1/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYfIxkMxf8Nc"
      },
      "source": [
        "## 2. Data Loading and Preprocessing\n",
        "\n",
        "[HuggingFaceM4/the_cauldron](https://huggingface.co/datasets/HuggingFaceM4/the_cauldron/viewer?views%5B%5D=ai2d) is a massive collection of 50 vision-language datasets spanning millions of rows across:\n",
        "\n",
        "1. General visual question answering\n",
        "2. OCR document understanding & text transcription\n",
        "3. Chart/figure understanding\n",
        "4. Table understanding\n",
        "5. Reasoning, logic, maths\n",
        "6. Textbook/academic questions\n",
        "7. Differences between 2 images\n",
        "8. Screenshot to code\n",
        "\n",
        "This dataset is a great resource for evaluating the image understanding capabilities of a vision language model as it gives us a wide range of tasks and image compositions to test on. It's size alone makes it particularly useful for training and validation.\n",
        "\n",
        "For now we will begin with General visual Q&A subset AI2D."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "27J-inULb4kn"
      },
      "outputs": [],
      "source": [
        "import daft\n",
        "\n",
        "df_raw = daft.read_huggingface(\"HuggingFaceM4/the_cauldron/ai2d\").collect()\n",
        "df_raw.show(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpxeWOz8P4pO"
      },
      "source": [
        "### Decoding Images into Daft's Image Type.\n",
        "\n",
        "Daft provides a simple way to decode images into its internal Image type. This allows you to use Daft's powerful image processing capabilities to preprocess your images before sending them to a model.\n",
        "\n",
        "Note: You can click on any cell to preview its contents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zFmi01VkPmUA"
      },
      "outputs": [],
      "source": [
        "from daft import col\n",
        "\n",
        "# Explode the list of images (usually just one image anyways)\n",
        "df_img = df_raw.explode(col(\"images\"))\n",
        "\n",
        "# Decode the image into daft.DataType.image()\n",
        "df_img = df_img.with_column(\"image_decoded\", col(\"images\")[\"bytes\"].decode_image())\n",
        "df_img.show(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJFwixJpQTAn"
      },
      "source": [
        "### Preprocessing the 'texts' column to extract Question, Choices, and Answer Columns\n",
        "\n",
        "Copy/Pasting an entry from the `texts` column yields an openai messages list of dicts of the form:\n",
        "\n",
        "```text\n",
        "[{\n",
        "    \"user\": \"\"\"Question:\n",
        "            \n",
        "        From the above food web diagram, what cause kingfisher to increase\n",
        "\n",
        "        Choices:\n",
        "            A. decrease in fish\n",
        "            B. decrease in water boatman\n",
        "            C. increase in fish\n",
        "            D. increase in algae\n",
        "\n",
        "        Answer with the letter.\"\"\",\n",
        "\n",
        "    \"assistant\": \"Answer: C\",\n",
        "    \"source\": \"AI2D\",\n",
        "}, ...]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HMW536eHUJ6Y"
      },
      "outputs": [],
      "source": [
        "from daft.functions import unnest\n",
        "\n",
        "# Explode the List of Dicts inside \"texts\" and unnest the resulting Struct into dedicated \"user\", \"assistant\", and \"source\" columns\n",
        "df_text = df_img.explode(col(\"texts\")).select(unnest(col(\"texts\")), \"image_decoded\")\n",
        "\n",
        "df_text.show(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brCTE8koRhfb"
      },
      "source": [
        "### Parsing Text with Regular Expressions\n",
        "\n",
        "We can also leverage Daft's built-in [regular expressions](https://docs.daft.ai/en/stable/api/functions/regexp_extract/#daft.functions.regexp_extract) to parse each assistant message to extract the answer.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JUD64rVnkWYE"
      },
      "outputs": [],
      "source": [
        "# Parsing \"assistant\" message to extract the answer\n",
        "df_prepped = df_text.with_column(\"answer\", col(\"assistant\").regexp_replace(\"Answer:\", \"\")).collect()\n",
        "df_prepped.show(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkVMvG9cg30_"
      },
      "source": [
        "## 3. Multimodal Structured Outputs with the `prompt` function\n",
        "\n",
        "Now we will move on to scaling our OpenAI client calls with Daft's new `prompt` function. Using a similar syntax to OpenAI client calls adapted for dataframes, we can quickly scale our structured output requests across the dataset.\n",
        "\n",
        "For more info see the [API docs](https://docs.daft.ai/en/stable/api/functions/prompt/#daft.functions.prompt), [User Guide](https://docs.daft.ai/en/stable/ai-functions/prompt/), & [Usage Patterns](https://github.com/Eventual-Inc/daft-examples/tree/main/usage_patterns/prompt)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mr-nh8scZyzs"
      },
      "outputs": [],
      "source": [
        "# First, we need to set the OpenAI Provider with our api_key and custom base_url that we set earlier.\n",
        "daft.set_provider(\"openai\", api_key=OPENAI_API_KEY, base_url=OPENAI_BASE_URL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t4G_yjdNdayW"
      },
      "outputs": [],
      "source": [
        "from daft import col\n",
        "from daft.functions import prompt\n",
        "from pydantic import BaseModel, Field\n",
        "import time\n",
        "\n",
        "# Lets keep our initial requests low to avoid spending too much $$$\n",
        "LIMIT = 50\n",
        "\n",
        "# We will also redefine our Pydantic Model to prioritize only the choice:\n",
        "class ChoiceResponse(BaseModel):\n",
        "    choice: str\n",
        "\n",
        "start = time.time()\n",
        "df = df_prepped.with_column(\n",
        "    \"result\",\n",
        "    prompt(\n",
        "        messages = [col(\"image_decoded\"), col(\"user\")],\n",
        "        system_message = \"Observe the attached image and respond to the multiple choice question with just the letter corresponding to the correct answer.\",\n",
        "        model = MODEL_ID,\n",
        "        use_chat_completions = True,\n",
        "        return_format=ChoiceResponse,\n",
        "    )\n",
        ").limit(LIMIT).collect()\n",
        "end = time.time()\n",
        "print(f\"Processed {df.count_rows()} rows in {end-start} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.show()"
      ],
      "metadata": {
        "id": "838neDZAhSXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zdOULnFwpfV5"
      },
      "outputs": [],
      "source": [
        "from daft import DataType\n",
        "import json\n",
        "\n",
        "@daft.func(return_dtype=DataType.struct({\"choice\": DataType.string()}))\n",
        "def normalize_output(result: str):\n",
        "    return json.loads(result)\n",
        "\n",
        "if OPENAI_BASE_URL == \"http://0.0.0.0:8000/v1\":\n",
        "    df = df.with_column(\"result\", normalize_vllm_output(col(\"result\"))).collect()\n",
        "\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef7NatQxFjt-"
      },
      "source": [
        "Now that we've run our structured output request, we can compare the model's answer to the correct answer to calculate a pass/fail rate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69XbKxrHqsF8"
      },
      "outputs": [],
      "source": [
        "df = df.with_column(\"is_correct\", col(\"result\")[\"choice\"].lstrip().rstrip() == col(\"answer\").lstrip().rstrip()) # strip whitespace\n",
        "\n",
        "pass_fail_rate = df.where(col(\"is_correct\")).count_rows() / df.count_rows()\n",
        "print(f\"Pass/Fail Rate: {pass_fail_rate}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVgjnPjfkEcz"
      },
      "source": [
        "## 6. Analysis: Image Input Ablation Study\n",
        "\n",
        "A simple accuracy score tells us *how often* the model is correct, but not *why*. To understand the contribution of image understanding to our model's performance, we'll conduct an **ablation study**—systematically removing the image input and comparing results.\n",
        "\n",
        "Our analysis proceeds in three stages:\n",
        "\n",
        "1. **Baseline Comparison**: Run the same prompts with and without images to establish accuracy deltas\n",
        "2. **Quadrant Classification**: Categorize each example into one of four outcomes:\n",
        "   - **Both Correct**: Model succeeds regardless of image (may indicate text-only solvable questions)\n",
        "   - **Image Helped**: Model only succeeds when given the image (true image understanding)\n",
        "   - **Image Hurt**: Model succeeds without the image but fails with it (potential visual confusion)\n",
        "   - **Both Incorrect**: Model fails regardless (harder questions or model limitations)\n",
        "3. **Case Inspection**: Examine specific examples from each quadrant to build intuition\n",
        "\n",
        "This methodology isolates the model's image understanding capability from its general reasoning ability, giving us actionable signal about where the model excels and struggles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CPKYUSpHrM3a"
      },
      "outputs": [],
      "source": [
        "# First lets investigate some of the Failures\n",
        "df.where(col(\"is_correct\") == False).select(\"user\", \"image_decoded\", \"answer\", \"result\").show(8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yg-tvu6eFjt-"
      },
      "source": [
        "These failures could have been caused by a variety of factors, but manually reviewing each of them one-by-one is intensive and time consuming."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pf2H3zDzFjt-"
      },
      "source": [
        "Moving beyond simple accuracy, we can learn a lot more about how strong our model's image understanding is by comparing our results with and without the image input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0nlo1hCzM1_l"
      },
      "outputs": [],
      "source": [
        "# How do these results compare without images?\n",
        "start = time.time()\n",
        "df = df.with_column(\n",
        "    \"result_no_image\",\n",
        "    prompt(\n",
        "        col(\"user\"),\n",
        "        system_message = \"Respond to the multiple choice question with just the letter corresponding to the correct answer.\",\n",
        "        model = MODEL_ID,\n",
        "        use_chat_completions=True,\n",
        "        **KWARGS,\n",
        "    )\n",
        ").with_column(\"result_no_image\", normalize_vllm_output(col(\"result\"))\n",
        ").with_column(\"is_correct_no_image\", col(\"result_no_image\")[\"choice\"] == col(\"answer\").lstrip().rstrip()\n",
        ").limit(LIMIT).collect()\n",
        "end = time.time()\n",
        "\n",
        "print(f\"Processed {df.count_rows()} rows in {end-start} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d2EJnsabBZpj"
      },
      "outputs": [],
      "source": [
        "pass_fail_rate_no_image = df.where(col(\"is_correct_no_image\")).count_rows() / df.count_rows()\n",
        "print(f\"Pass/Fail Rate: \\n With Image: {pass_fail_rate} \\n Without Image: {pass_fail_rate_no_image} \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEUsdFYdFjuE"
      },
      "source": [
        "- Now that we've got our pass fail rates between our tests with and without images, how did the accuracy compare?\n",
        "- Given your results is it clear whether or not the image helped or hurt the model's performance?\n",
        "- Lets dive into the details to gain a better understanding of our model's performance by investigating further..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZELb70SyFjuE"
      },
      "outputs": [],
      "source": [
        "# Lets assign a label to each row and investigate the results.\n",
        "df = df.with_column(\"id\", daft.functions.monotonically_increasing_id())\n",
        "df.select(\"id\", \"user\", \"image_decoded\", \"answer\", \"result\", \"result_no_image\", \"is_correct\", \"is_correct_no_image\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EiAfNFHTFjuE"
      },
      "outputs": [],
      "source": [
        "# Where did the model have a different answer with and without an image input?\n",
        "df_img_diff = df.where(col(\"result\")[\"choice\"] != col(\"result_no_image\")[\"choice\"]).collect()\n",
        "df_img_diff.count_rows()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WakAX8h9FjuE"
      },
      "outputs": [],
      "source": [
        "# What were the differences in correctness?\n",
        "df_correct_diff = df.where(col(\"is_correct\") != col(\"is_correct_no_image\")).collect()\n",
        "df_correct_diff.count_rows()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kn2SZo6PFjuF"
      },
      "outputs": [],
      "source": [
        "# What is the combination of both of these?\n",
        "from daft.functions import when\n",
        "\n",
        "df_classified = df.with_column(\n",
        "    \"classification\",\n",
        "    when((col(\"is_correct\") == True) & (col(\"is_correct_no_image\") == True), \"Both Correct\")\n",
        "    .when((col(\"is_correct\") == True) & (col(\"is_correct_no_image\") == False), \"Image Helped\")\n",
        "    .when((col(\"is_correct\") == False) & (col(\"is_correct_no_image\") == True), \"Image Hurt\")\n",
        "    .otherwise(\"Both Incorrect\")\n",
        ")\n",
        "\n",
        "# View the counts for each quadrant\n",
        "df_classified.groupby(\"classification\").count().select(\"classification\", col(\"id\").alias(\"count\")).show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSw-YfhzFjuF"
      },
      "source": [
        "Now that we have a better idea of the distribution of our results, lets investigate specific cases where the image helped or hurt the model's performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_9CNMK9KFjuF"
      },
      "outputs": [],
      "source": [
        "# Inspect specific cases where the image helped\n",
        "df_classified.where(col(\"classification\") == \"Image Helped\").select(\"user\", \"image_decoded\", \"answer\", col(\"result\")[\"choice\"], col(\"result_no_image\")[\"choice\"]).show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BA83PdsYFjuF"
      },
      "outputs": [],
      "source": [
        "# Inspect specific cases where including the image hurt the model's performance\n",
        "df_classified.where(col(\"classification\") == \"Image Hurt\").select(\"user\", \"image_decoded\", \"answer\", \"result\", \"result_no_image\").show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fe7awB1NFjuF"
      },
      "source": [
        "## LLM-as-a-Judge Evaluation\n",
        "\n",
        "Now that we've identified *where* our model failed through ablation, let's use **VLM-as-a-Judge** to understand *why*. As introduced earlier, we're combining **Reference-Guided Grading** (comparing against the known correct answer) with **diagnostic failure attribution** (analyzing the root cause of errors).\n",
        "\n",
        "For each failure case, our judge will inspect the image and provide:\n",
        "- **Reasoning**: Why the model chose its answer\n",
        "- **Hypothesis**: What caused the divergence from the correct answer  \n",
        "- **Attribution**: Whether the failure stems from the question or the image\n",
        "\n",
        "This diagnostic feedback can be integrated into experiment tracking systems to systematically improve prompts and model configurations over time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uBkFX88-FjuF"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Have an LLM Judge run a post Mortem\n",
        "judge_template = format(\n",
        "    \"Referencing the attached image, hypothesize why the model under evaluation chose <choice_with_image>{}</choice_with_image> and <choice_no_image>{}</choice_no_image> to the <question>{}</question> where the correct answer is supposed to be <correct_answer>{}</correct_answer>.\",\n",
        "    col(\"result\")[\"choice\"],\n",
        "    col(\"result_no_image\")[\"choice\"],\n",
        "    col(\"user\"),\n",
        "    col(\"answer\")\n",
        ")\n",
        "\n",
        "judge_system_prompt = \"\"\"\n",
        "You are an impartial judge reviewing the results of a visual question and answer benchmark of a vision language model.\n",
        "Focusing on the discrepancy between the model's answer with and without the image contrasted against the correct answer, inspect the attached image and provide high-signal feedback why the model chose the answer it did.\n",
        "Do not propose improvements.\n",
        "Your hypothesis should be grounded in evaluating image understanding, improving the models ability to reason about the image, and not the models ability to reason about the text.\n",
        "Specifically, your feedback should only improve accuracy scores when the image is attached, and should not improve scores when the image is not attached.\n",
        "This is a safe space for you to express your thoughts and insights without fear of spec-gaming.\n",
        "\"\"\"\n",
        "\n",
        "class JudgeResponse(BaseModel):\n",
        "    reasoning: str = Field(..., description=\"Provide the reasoning for why the model chose the answer it did.\")\n",
        "    hypothesis: str = Field(..., description=\"Provide the hypothesis for why the model's choices diverged from the correct answer.\")\n",
        "    attribution: str = Field(..., description=\"Concisely attribute a specific aspect of the image or question that may have led to the model's choices diverging from the correct answer.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aoxKQetcFjuF"
      },
      "outputs": [],
      "source": [
        "from daft import lit\n",
        "# Filter for the Failures\n",
        "df_failures = df_classified.where((col(\"classification\") == \"Image Hurt\") | (col(\"classification\") == \"Both Incorrect\"))\n",
        "\n",
        "# Run the Evaluation\n",
        "df_judge = (\n",
        "    df_failures\n",
        "    .with_column(\n",
        "        \"judge\",\n",
        "        prompt(\n",
        "            messages = [col(\"image_decoded\"), judge_template],\n",
        "            system_message = judge_system_prompt,\n",
        "            model = MODEL_ID,\n",
        "            use_chat_completions = True,\n",
        "            return_format = JudgeResponse,\n",
        "        )\n",
        "    )\n",
        ").collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UAI-37yuFjuF"
      },
      "outputs": [],
      "source": [
        "# Lets review the Judge's outputs:\n",
        "df_judge.select(\"user\", \"result\", \"answer\", \"image_decoded\", unnest(col(\"judge\"))).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mz4-y_GXFjuF"
      },
      "source": [
        "Keep in mind that having a large number of tests is critical to ensuring that your vision language model isn't gaming the system by simply memorizing the answers. It is always considered best practice to split your training and validation data into separate datasets, commonly called a test/train split."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eALCQifQPmfp"
      },
      "source": [
        "# Putting everything together\n",
        "\n",
        "Now that we have walked through implementing this image understanding evaluation interactively, lets combine all of our code into a single pipeline so we can take full advantage of lazy evaluation and provide opportunities for future extensibility and re-use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TObFYF7xeKFJ"
      },
      "outputs": [],
      "source": [
        "from daft.functions import format\n",
        "from pydantic import BaseModel, Field\n",
        "import os\n",
        "\n",
        "OPENAI_API_KEY = os.environ.get(\"OPENROUTER_API_KEY\")\n",
        "OPENAI_BASE_URL = \"https://openrouter.ai/api/v1/\"\n",
        "MODEL_ID = \"qwen/qwen3-vl-8b-instruct\"        # vLLM & OpenRouter\n",
        "DATASET_URI = \"HuggingFaceM4/the_cauldron\"\n",
        "SUBSET = \"ai2d\"\n",
        "LIMIT = 100\n",
        "BASE_PROMPT = \"Respond to the multiple choice question with just the letter corresponding to the correct answer.\"\n",
        "IMAGE_PROMPT = \"Reference the attached image\"\n",
        "KWARGS = {\n",
        "    \"temperature\": 0.1,\n",
        "}\n",
        "\n",
        "class ChoiceResponse(BaseModel):\n",
        "    choice: str = Field(..., description=\"Provide the letter of the correct choice with no other text ie: F\")\n",
        "\n",
        "JUDGE_TEMPLATE = format(\n",
        "    \"Referencing the attached image, hypothesize why the model under evaluation chose <choice_with_image>{}</choice_with_image> and <choice_no_image>{}</choice_no_image> to the <question>{}</question> where the correct answer is supposed to be <correct_answer>{}</correct_answer>.\",\n",
        "    col(\"result\")[\"choice\"],\n",
        "    col(\"result_no_image\")[\"choice\"],\n",
        "    col(\"user\"),\n",
        "    col(\"answer\")\n",
        ")\n",
        "\n",
        "JUDGE_SYSTEM_PROMPT = \"\"\"\n",
        "You are an impartial judge reviewing the results of a visual question and answer benchmark of a vision language model.\n",
        "Focusing on the discrepancy between the model's answer with and without the image contrasted against the correct answer, inspect the attached image and provide high-signal feedback why the model chose the answer it did.\n",
        "Do not propose improvements.\n",
        "Your hypothesis should be grounded in evaluating image understanding, improving the models ability to reason about the image, and not the models ability to reason about the text.\n",
        "Specifically, your feedback should only improve accuracy scores when the image is attached, and should not improve scores when the image is not attached.\n",
        "This is a safe space for you to express your thoughts and insights without fear of spec-gaming.\n",
        "\"\"\"\n",
        "\n",
        "class JudgeResponse(BaseModel):\n",
        "    reasoning: str = Field(..., description=\"Provide the reasoning for why the model chose the answer it did.\")\n",
        "    hypothesis: str = Field(..., description=\"Provide the hypothesis for why the model's choices diverged from the correct answer.\")\n",
        "    attribution: str = Field(..., description=\"Attribute a specific aspect of the image or question that may have led to the model's choices diverging from the correct answer.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olGKZeZSFjuF"
      },
      "source": [
        "We can break each of our steps down into functions for reusability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ABj0WRugFjuF"
      },
      "outputs": [],
      "source": [
        "import daft\n",
        "from daft import col, lit\n",
        "from daft.functions import prompt, when, format, monotonically_increasing_id, unnest\n",
        "\n",
        "# Read the Dataset\n",
        "df_raw = daft.read_huggingface(f\"HuggingFaceM4/the_cauldron/{SUBSET}\")\n",
        "\n",
        "# Preprocess the dataset\n",
        "df_prep = (\n",
        "    df_raw\n",
        "    # Prepare Images\n",
        "    .explode(\"images\")\n",
        "    .with_column(\"image_decoded\", col(\"images\").struct.get(\"bytes\").decode_image())\n",
        "    # Prepare Text\n",
        "    .explode(\"texts\")\n",
        "    .select(unnest(col(\"texts\")), \"image_decoded\")\n",
        "    # Extract Answer Letter\n",
        "    .with_column(\"answer\", col(\"assistant\").regexp_replace(\"Answer: \", \"\"))\n",
        ")\n",
        "\n",
        "\n",
        "df_run = (\n",
        "    df_prep\n",
        "    # Run Structured Output on the images + text\n",
        "    .with_column(\n",
        "        \"result\",\n",
        "        prompt(\n",
        "            messages = [col(\"image_decoded\"), col(\"user\")],\n",
        "            system_message = IMAGE_PROMPT + \" \" + BASE_PROMPT,\n",
        "            model = MODEL_ID,\n",
        "            use_chat_completions = True,\n",
        "            return_format = ChoiceResponse,\n",
        "            **KWARGS,\n",
        "        )\n",
        "    )\n",
        "    # Evaluate Correctness\n",
        "    .with_column(\n",
        "        \"is_correct\",\n",
        "        col(\"result\")[\"choice\"] == col(\"answer\"),\n",
        "    )\n",
        "    # Run Structured Output on the text only\n",
        "    .with_column(\n",
        "        \"result_no_image\",\n",
        "        prompt(\n",
        "            messages = col(\"user\"),\n",
        "            system_message = BASE_PROMPT,\n",
        "            model = MODEL_ID,\n",
        "            provider = \"openai\",\n",
        "            use_chat_completions = True,\n",
        "            return_format = ChoiceResponse,\n",
        "        )\n",
        "    )\n",
        "    # Evaluate Correctness\n",
        "    .with_column(\n",
        "        \"is_correct_no_image\",\n",
        "        col(\"result_no_image\")[\"choice\"] == col(\"answer\"),\n",
        "    )\n",
        ")\n",
        "\n",
        "# Analyze the results\n",
        "df_analysis = (\n",
        "    df_run\n",
        "    .with_column(\"id\", monotonically_increasing_id())\n",
        "    .with_column(\n",
        "        \"classification\",\n",
        "        when((col(\"is_correct\") == True) & (col(\"is_correct_no_image\") == True), \"Both Correct\")\n",
        "        .when((col(\"is_correct\") == True) & (col(\"is_correct_no_image\") == False), \"Image Helped\")\n",
        "        .when((col(\"is_correct\") == False) & (col(\"is_correct_no_image\") == True), \"Image Hurt\")\n",
        "        .otherwise(\"Both Incorrect\")\n",
        "    )\n",
        ")\n",
        "\n",
        "\n",
        "# Grab the rows where the image hurt or both incorrect\n",
        "df_failures = df_analysis.where((col(\"classification\") == lit(\"Image Hurt\")) | (col(\"classification\") == lit(\"Both Incorrect\")))\n",
        "\n",
        "# Run LLM-as-a-Judge\n",
        "df_judge = (\n",
        "    df_failures\n",
        "    .with_column(\n",
        "        \"judge\",\n",
        "        prompt(\n",
        "            messages = [col(\"image_decoded\"), JUDGE_TEMPLATE],\n",
        "            system_message = JUDGE_SYSTEM_PROMPT,\n",
        "            model = MODEL_ID,\n",
        "            provider = \"openai\",\n",
        "            use_chat_completions = True,\n",
        "            return_format = JudgeResponse,\n",
        "        )\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRQ8hwIcFjuF"
      },
      "source": [
        "# Executing the Pipeline\n",
        "\n",
        "Since our pipeline is lazy, we can break down our execution as needed. In this scenario, we will want to materialize `df_run` and execute our `df_analysis` and `df_judge`in a seperate step to minimize recomputation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j5AomZwjFjuF"
      },
      "outputs": [],
      "source": [
        "# Execute the Run Step\n",
        "df_run = df_run.limit(LIMIT).collect()\n",
        "df_run.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ujS9OhhlFjuF"
      },
      "outputs": [],
      "source": [
        "# Execute the Analysis Step\n",
        "df_analysis = df_analysis.limit(LIMIT).collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jrdtxCW7FjuF"
      },
      "outputs": [],
      "source": [
        "# Execute the Judge Step\n",
        "df_judge = df_judge.limit(LIMIT).collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lx6XT3evFjuF"
      },
      "source": [
        "### Showing Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NJvG7cXIFjuF"
      },
      "outputs": [],
      "source": [
        "# Show Counts of the quadrant\n",
        "df_counts = df_analysis.groupby(\"classification\").count().select(\"classification\", col(\"id\").alias(\"count\")).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A98SLoXALhxR"
      },
      "outputs": [],
      "source": [
        "# Show which ids are in each quadrant\n",
        "df_ids = df_analysis.groupby(\"classification\").agg_list(\"id\").select(\"classification\", col(\"id\").alias(\"ids\")).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mFdEC8JDMj8E"
      },
      "outputs": [],
      "source": [
        "df_judge.select(\"user\", \"result\", \"answer\", \"image_decoded\", unnest(col(\"judge\"))).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkhxBeusFjuF"
      },
      "source": [
        "### Persisting Results\n",
        "\n",
        "Finally we can persist our results to a table for future analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0v6x19XxNyJa"
      },
      "outputs": [],
      "source": [
        "df_run.write_parquet(\".data/the_cauldron_image_ablation_study.parquet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ohn2kqEAFjuF"
      },
      "outputs": [],
      "source": [
        "df_analysis.write_parquet(\".data/the_cauldron_image_ablation_study_analysis.parquet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bZ9V24phFjuF"
      },
      "outputs": [],
      "source": [
        "df_judge.write_parquet(\".data/the_cauldron_image_ablation_study_judge.parquet\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbzOoC62uDu1"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "In this notebook we explored how to evaluate Qwen-3-VL's image understanding using a subset from HuggingFace's TheCauldron Dataset. The AI2D subset we used is just one of a massive collection of 50 vision-language datasets that can be used for evaluating or training vision language models totaling millions of rows. You can also leverage this pipeline to evaluate model performance across sampling parameters or model variants. Please note that not all Qwen-3-VL models support image inputs, and leveraging datasets outside of *The Cauldron* would require different preprocessing stages.\n",
        "\n",
        "A natural next step would be to parallelize this pipeline across multiple datasets leveraging multiple GPUs. In this scenario, leveraging a managed provider like OpenRouter isn't feasible. With [Daft Cloud](https://daft.ai/cloud) you can run Qwen-3-VL on as much data as you want with no rate limits or GPU configuration headaches.\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "\n",
        "**1. Multi-Dataset Evaluation**\n",
        "\n",
        "Extend evaluation across all 50 subsets of The Cauldron to build a comprehensive benchmark:\n",
        "\n",
        "```python\n",
        "subsets = [\"ai2d\", \"chartqa\", \"docvqa\", \"infographicvqa\", ...]\n",
        "for subset in subsets:\n",
        "    df = pipeline(f\"HuggingFaceM4/the_cauldron/{subset}\")\n",
        "    df.write_parquet(f\"results/{subset}.parquet\")\n",
        "```\n",
        "\n",
        "**2. Distributed Execution with Ray**\n",
        "\n",
        "For large-scale runs, transition to Daft's distributed runner, [Flotilla](https://www.daft.ai/blog/introducing-flotilla-simplifying-multimodal-data-processing-at-scale) for distributed compute or Daft Cloud for fully managed service.\n",
        "\n",
        "```python\n",
        "import daft\n",
        "daft.set_runner_ray()  # Daft orchestrates Ray automatically\n",
        "```\n",
        "\n",
        "**3. Experiment Tracking Integration**\n",
        "\n",
        "Wire the judge feedback into MLflow, Weights & Biases, or similar tools to track how prompt/parameter changes affect the accuracy quadrants over time.\n",
        "\n",
        "**4. Reinforcement Learning with Verifiable Rewards (RLVR)**\n",
        "\n",
        "The pipeline we built produces exactly what's needed for RLVR training loops:\n",
        "- **Verifiable rewards**: The `is_correct` column provides a binary reward signal—no human labeling required\n",
        "- **Diagnostic signal**: The judge's `attribution` field (\"question\" vs \"image\") can inform reward shaping, penalizing failures caused by poor image understanding more heavily\n",
        "- **Scalable generation**: Daft can generate millions of (prompt, response, reward) tuples for frameworks like [TRL](https://huggingface.co/docs/trl), [OpenRLHF](https://github.com/OpenRLHF/OpenRLHF), or [veRL](https://github.com/volcengine/verl)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "gqjRsfxH891x",
        "J_wIalJJs0ki"
      ],
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}