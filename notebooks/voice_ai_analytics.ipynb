{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simplifying Voice AI Analytics with Daft\n",
    "\n",
    "## Transcription, Summaries, and Embeddings at Scale\n",
    "\n",
    "This tutorial walks through how to build a Voice AI analytics pipeline using Daft and Faster-Whisper from raw audio to searchable, multilingual transcripts. You'll learn how to:\n",
    "\n",
    "- Transcribe long-form audio using Faster-Whisper with built-in VAD for speaker segmentation\n",
    "- Use Daft's dataframe engine to orchestrate and parallelize multimodal processing at scale\n",
    "- Generate summaries, translations, and embeddings directly from transcripts\n",
    "\n",
    "In short, Daft simplifies multimodal AI pipelines letting developers process, enrich, and query audio data with the same ease as tabular data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Voice AI\n",
    "\n",
    "Behind every AI meeting note, podcast description, and voice agent lies an AI pipeline that transcribes raw audio into text and enriches those transcripts to make it retrieval performant for downstream applications.\n",
    "\n",
    "Voice AI encompasses a broad range of tasks:\n",
    "\n",
    "1. **Voice Activity Detection (VAD)** - Detects when speech is present in an audio signal\n",
    "2. **Speech-to-Text (STT)** - The core method of extracting transcriptions from audio\n",
    "3. **Speaker Diarization** - Identifies and segments which speaker is talking when\n",
    "4. **LLM Text Generation** - For summaries, translations, and more\n",
    "5. **Text-to-Speech (TTS)** - Brings LLM responses and translations to life in spoken form\n",
    "6. **Turn Detection** - Useful for live voice chat\n",
    "\n",
    "In this tutorial we will focus on **Speech-to-Text (STT)** and **LLM Text Generation**, exploring common techniques for preprocessing and enriching human speech from audio to support downstream applications like meeting summaries, short-form editing, and embeddings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges in Processing Audio for AI Pipelines\n",
    "\n",
    "Audio is inherently different from traditional analytics processing. Most multimodal AI workloads require some level of preprocessing before inference, but since audio isn't stored in neat rows and columns in a table, running frontier models on audio data comes with some extra challenges.\n",
    "\n",
    "**Before we can run our STT models on audio data we'll need to:**\n",
    "- Read and preprocess raw audio files into a form that the model can receive\n",
    "- Handle memory constraints (e.g., one hour of 48 kHz/24-bit stereo audio can be 518 MB)\n",
    "- Decode, buffer, and resample audio files into chunks\n",
    "- Manage streaming architectures with message queues for back pressure\n",
    "\n",
    "**Traditional approaches face challenges:**\n",
    "- Scaling parallelism requires multiprocessing/threading (error-prone, GIL limitations)\n",
    "- Memory management needs custom generators/lazy loading (overflows common)\n",
    "- Pipelining stages are hardcoded (modifications tedious, no retry mechanisms)\n",
    "- Storing and querying outputs requires custom scripts (performance degradation)\n",
    "\n",
    "**Daft solves these issues by:**\n",
    "- Providing a unified dataframe interface for multimodal data\n",
    "- Handling distributed parallelism automatically\n",
    "- Managing memory efficiently with Apache Arrow format\n",
    "- Enabling lazy evaluation for optimal query planning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "Let's start by importing the necessary libraries and setting up our environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '.venv (Python 3.11.14)' requires the ipykernel package.\n",
      "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/Users/everettkleven/git/work/daft-examples-1/.venv/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "! uv pip install daft faster-whisper soundfile sentence-transformers openai python-dotenv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import asdict\n",
    "import os\n",
    "\n",
    "import daft\n",
    "from daft import DataType, col\n",
    "from daft.functions import format, file, unnest\n",
    "from daft.functions.ai import prompt, embed_text\n",
    "from daft.ai.openai.provider import OpenAIProvider\n",
    "from faster_whisper import WhisperModel, BatchedInferencePipeline\n",
    "\n",
    "from transcription_schema import TranscriptionResult\n",
    "\n",
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Constants and Configuration\n",
    "\n",
    "Let's define the parameters we'll use throughout this tutorial.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Constants\n",
    "SAMPLE_RATE = 16000\n",
    "DTYPE = \"float32\"\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# Define Parameters\n",
    "SOURCE_URI = \"hf://datasets/Eventual-Inc/sample-files/audio/*.mp3\"\n",
    "DEST_URI = \".data/voice_ai_analytics\"\n",
    "LLM_MODEL_ID = \"openai/gpt-oss-120b\"\n",
    "EMBEDDING_MODEL_ID = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "CONTEXT = \"Daft: Unified Engine for Data Analytics, Engineering & ML/AI (github.com/Eventual-Inc/Daft) YouTube channel video. Transcriptions can have errors like 'DAF' referring to 'Daft'.\"\n",
    "PRINT_SEGMENTS = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a High-Performance Transcription Pipeline with Faster-Whisper\n",
    "\n",
    "Faster-Whisper comes with built-in VAD from Silero for segmenting long-form audio into neat chunks. This makes it so we don't need to worry about the length of video or handle any windowing ourselves since Whisper only operates over 30 sec chunks. We also want to take full advantage of faster-whisper's `BatchedInferencePipeline` to improve our throughput.\n",
    "\n",
    "### Creating the FasterWhisperTranscriber Class\n",
    "\n",
    "We'll define a `FasterWhisperTranscriber` class and decorate it with `@daft.cls()`. This converts any standard Python class into a distributed massively parallel user-defined-function, enabling us to take full advantage of Daft's rust-backed performance.\n",
    "\n",
    "**Key design decisions:**\n",
    "- We separate model loading from inference in the `__init__` method\n",
    "- Models can easily reach multiple GB in size, so we initialize during class instantiation to avoid repeated downloads\n",
    "- We input a `daft.File` and return a dictionary that will be materialized as a `daft.DataType.struct()`\n",
    "- Faster-whisper supports reading files directly, so we use `daft.File` for simplified preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@daft.cls()\n",
    "class FasterWhisperTranscriber:\n",
    "    def __init__(self, model=\"distil-large-v3\", compute_type=\"float32\", device=\"auto\"):\n",
    "        self.model = WhisperModel(model, compute_type=compute_type, device=device)\n",
    "        self.pipe = BatchedInferencePipeline(self.model)\n",
    "\n",
    "    @daft.method(return_dtype=TranscriptionResult)\n",
    "    def transcribe(self, audio_file: daft.File):\n",
    "        \"\"\"Transcribe Audio Files with Voice Activity Detection (VAD) using Faster Whisper\"\"\"\n",
    "        with audio_file.to_tempfile() as tmp:\n",
    "            segments_iter, info = self.pipe.transcribe(\n",
    "                str(tmp.name),\n",
    "                vad_filter=True,\n",
    "                vad_parameters=dict(min_silence_duration_ms=500, speech_pad_ms=200),\n",
    "                word_timestamps=True,\n",
    "                without_timestamps=False,\n",
    "                temperature=0,\n",
    "                batch_size=BATCH_SIZE,\n",
    "            )\n",
    "            segments = [asdict(seg) for seg in segments_iter]\n",
    "            text = \" \".join([seg[\"text\"] for seg in segments])\n",
    "\n",
    "            return {\"transcript\": text, \"segments\": segments, \"info\": asdict(info)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up OpenAI Provider for LLM Operations\n",
    "\n",
    "We'll use OpenRouter as our LLM provider for summaries and translations. Let's configure it:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an OpenAI provider, attach, and set as the default\n",
    "openrouter_provider = OpenAIProvider(\n",
    "    name=\"OpenRouter\",\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    api_key=os.environ.get(\"OPENROUTER_API_KEY\"),\n",
    ")\n",
    "daft.attach_provider(openrouter_provider)\n",
    "daft.set_provider(\"OpenRouter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Daft's DataFrame Interface\n",
    "\n",
    "Before we dive into transcription, let's understand why Daft's dataframe interface is powerful:\n",
    "\n",
    "1. **Tabular Operations**: Perform traditional operations within a managed data model - harder to mess up data structures\n",
    "2. **Automatic Parallelism**: Abstract complexity of orchestrating processing for distributed parallelism - maximum CPU and GPU utilization by default\n",
    "3. **Lazy Evaluation**: Operations aren't materialized until we invoke collection - enables query optimization and decouples transformations from load\n",
    "\n",
    "Daft's execution engine runs on a push-based processing model, enabling the engine to optimize each operation by planning everything from query through the logic and finally writing to disk.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Transcription\n",
    "\n",
    "Now let's transcribe our audio files. We'll:\n",
    "1. Discover audio files from the source URI\n",
    "2. Wrap paths as `daft.File` objects\n",
    "3. Transcribe using our FasterWhisperTranscriber\n",
    "4. Unpack the results into separate columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Transcription UDF\n",
    "fwt = FasterWhisperTranscriber()\n",
    "\n",
    "# Transcribe the audio files\n",
    "df_transcript = (\n",
    "    # Discover the audio files\n",
    "    daft.from_glob_path(SOURCE_URI)\n",
    "    # Wrap the path as a daft.File\n",
    "    .with_column(\"audio_file\", file(col(\"path\")))\n",
    "    # Transcribe the audio file with Voice Activity Detection (VAD) using Faster Whisper\n",
    "    .with_column(\"result\", fwt.transcribe(col(\"audio_file\")))\n",
    "    # Unpack Results\n",
    "    .select(\"path\", \"audio_file\", unnest(col(\"result\")))\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the transcript\n",
    "df_transcript.select(\n",
    "    \"path\",\n",
    "    \"info\",\n",
    "    \"transcript\",\n",
    "    \"segments\",\n",
    ").show(format=\"fancy\", max_width=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We've successfully transcribed our audio files. The dataframe now contains:\n",
    "- `path`: The source file path\n",
    "- `transcript`: The full transcription text\n",
    "- `segments`: A list of transcription segments with timestamps\n",
    "- `info`: Metadata about the transcription (language, duration, etc.)\n",
    "\n",
    "## Step 2: Summarization\n",
    "\n",
    "Moving on to our downstream enrichment stages, summarization is a common and simple means of leveraging an LLM for publishing, socials, or search. With Daft, generating a summary from your transcripts is as simple as adding a column.\n",
    "\n",
    "We'll also demonstrate how easy it is to add translations - since all the data is organized and accessible, we just need to declare what we want!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize the transcripts and translate to Chinese\n",
    "df_summaries = (\n",
    "    df_transcript\n",
    "    # Summarize the transcripts\n",
    "    .with_column(\n",
    "        \"summary\",\n",
    "        prompt(\n",
    "            format(\n",
    "                \"Summarize the following transcript from a YouTube video belonging to {}: \\n {}\",\n",
    "                daft.lit(CONTEXT),\n",
    "                col(\"transcript\"),\n",
    "            ),\n",
    "            model=LLM_MODEL_ID,\n",
    "        ),\n",
    "    ).with_column(\n",
    "        \"summary_chinese\",\n",
    "        prompt(\n",
    "            format(\n",
    "                \"Translate the following text to Simplified Chinese: <text>{}</text>\",\n",
    "                col(\"summary\"),\n",
    "            ),\n",
    "            system_message=\"You will be provided with a piece of text. Your task is to translate the text to Simplified Chinese exactly as it is written. Return the translated text only, no other text or formatting.\",\n",
    "            model=LLM_MODEL_ID,\n",
    "        ),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the summaries and the transcript\n",
    "df_summaries.select(\n",
    "    \"path\",\n",
    "    \"transcript\",\n",
    "    \"summary\",\n",
    "    \"summary_chinese\",\n",
    ").show(format=\"fancy\", max_width=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent! We now have summaries in both English and Chinese. This demonstrates how easy it is to add multilingual support to your pipeline.\n",
    "\n",
    "## Step 3: Generating Subtitles\n",
    "\n",
    "A common downstream task is preparing subtitles. Since our segments come with start and end timestamps, we can easily add another section to our Voice AI pipeline for translation. We'll explode the segments (one row per segment) and translate each segment to Simplified Chinese.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explode the segments, embed, and translate to simplified Chinese for subtitles\n",
    "df_segments = (\n",
    "    df_transcript.explode(\"segments\")\n",
    "    .select(\n",
    "        \"path\",\n",
    "        unnest(col(\"segments\")),\n",
    "    )\n",
    "    .with_column(\n",
    "        \"segment_text_chinese\",\n",
    "        prompt(\n",
    "            format(\n",
    "                \"Translate the following text to Simplified Chinese: <text>{}</text>\",\n",
    "                col(\"text\"),\n",
    "            ),\n",
    "            system_message=\"You will be provided with a transcript segment. Your task is to translate the text to Simplified Chinese exactly as it is written. Return the translated text only, no other text or formatting.\",\n",
    "            model=LLM_MODEL_ID,\n",
    "        ),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the segments and translations\n",
    "df_segments.select(\n",
    "    \"path\",\n",
    "    \"text\",\n",
    "    \"segment_text_chinese\",\n",
    ").show(format=\"fancy\", max_width=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect! These segments can now be used to make content more accessible for wider audiences, which is a great way to increase reach. Each segment has:\n",
    "- Original text with timestamps (`start`, `end`)\n",
    "- Chinese translation\n",
    "- Ready to use for subtitle generation\n",
    "\n",
    "## Step 4: Embedding Segments for Later Retrieval\n",
    "\n",
    "Our final stage is embeddings. If you're going through the trouble of transcription, you might as well make that content available as part of your knowledge base. Meeting notes might not be the most advanced AI use-case anymore, but it still provides immense value for tracking decisions and key moments in discussions.\n",
    "\n",
    "Adding an embeddings stage is as simple as calling `embed_text()`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed the segments\n",
    "df_segments = df_segments.with_column(\n",
    "    \"segment_embeddings\",\n",
    "    embed_text(\n",
    "        col(\"text\"),\n",
    "        provider=\"transformers\",\n",
    "        model=EMBEDDING_MODEL_ID,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the segments with embeddings\n",
    "df_segments.select(\n",
    "    \"path\",\n",
    "    \"text\",\n",
    "    \"segment_embeddings\",\n",
    ").show(format=\"fancy\", max_width=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent! Daft's native embedding DataType intelligently stores embedding vectors for you, regardless of their size. Now you have:\n",
    "- Transcript segments with timestamps\n",
    "- Embeddings ready for semantic search\n",
    "- Translations for multilingual support\n",
    "\n",
    "## Summary\n",
    "\n",
    "We've successfully built a complete Voice AI Analytics pipeline that:\n",
    "\n",
    "1. ✅ **Ingests** a directory of audio files\n",
    "2. ✅ **Transcribes** speech to text using Faster-Whisper with VAD\n",
    "3. ✅ **Generates** summaries from the transcripts\n",
    "4. ✅ **Translates** transcript segments to Chinese for subtitles\n",
    "5. ✅ **Embeds** transcriptions for future semantic search\n",
    "\n",
    "## Extensions and Next Steps\n",
    "\n",
    "From here there are several directions you could take:\n",
    "\n",
    "### 1. **Q/A Chatbot**\n",
    "Leverage the embeddings to host a Q/A chatbot that enables listeners to engage with content across episodes:\n",
    "- \"What did Sam Harris say about free will in episode 267?\"\n",
    "- \"Find all discussions about AI safety across my subscribed podcasts\"\n",
    "\n",
    "### 2. **Recommendation Engine**\n",
    "Build recommendation engines that surface hidden gems based on semantic similarity rather than just metadata tags.\n",
    "\n",
    "### 3. **Dynamic Highlight Reels**\n",
    "Create dynamic highlight reels that auto-generate shareable clips based on sentiment spikes and topic density.\n",
    "\n",
    "### 4. **RAG Workflow**\n",
    "Leverage Daft's `cosine_distance` function to put together a full RAG (Retrieval-Augmented Generation) workflow for an interactive experience.\n",
    "\n",
    "### 5. **Analytics Dashboards**\n",
    "Use the same tooling to power analytics dashboards showcasing trending topics, or supply content for automated newsletters.\n",
    "\n",
    "Since everything you store is queryable and performant, the only limit is your imagination!\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "- **Daft simplifies multimodal AI pipelines** - Process, enrich, and query audio data with the same ease as tabular data\n",
    "- **Automatic parallelism** - Maximum CPU and GPU utilization by default\n",
    "- **Lazy evaluation** - Optimized query planning and efficient resource usage\n",
    "- **Easy extensibility** - Adding new stages (summaries, translations, embeddings) is just another line of code\n",
    "- **No manual orchestration** - No need to handle VAD, batching, or multiprocessing manually\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "At Eventual, we're simplifying multimodal AI so you don't have to. Managing voice AI pipelines or processing thousands of hours of podcast audio ultimately comes down to a few universal needs:\n",
    "\n",
    "- **Transcripts** so your content is accessible and searchable\n",
    "- **Summaries** so your listeners can skim and find what matters\n",
    "- **Translations** so you can localize your content to your audience\n",
    "- **Embeddings** so people can ask questions like \"Which episode talked about reinforcement learning?\"\n",
    "\n",
    "Traditionally, delivering all of this meant juggling multiple tools, data formats, and scaling headaches — a brittle setup that doesn't grow with your workload. With Daft, you get one unified engine to process, store, and query multimodal data efficiently.\n",
    "\n",
    "**Fewer moving parts means fewer failure points, less debugging, and a much shorter path from raw audio to usable insights.**\n",
    "\n",
    "---\n",
    "\n",
    "*For more examples and to get help, check out:*\n",
    "- **GitHub**: [github.com/Eventual-Inc/Daft](https://github.com/Eventual-Inc/Daft)\n",
    "- **Slack**: Join our community for support and discussions\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
